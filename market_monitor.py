import pandas as pd
import numpy as np
import re
import os
import logging
from datetime import datetime, timedelta, time
import random
from io import StringIO
import requests
import tenacity
import concurrent.futures
import time as time_module
import json
from typing import Dict, Optional, Tuple

# é…ç½®å¤–éƒ¨åŒ– - ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°
CONFIG_FILE = 'market_monitor_config.json'

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºé»˜è®¤é…ç½®"""
    default_config = {
        "network": {
            "max_workers": 5,
            "retry_attempts": 5,
            "retry_wait_seconds": 10,
            "request_timeout": 30,
            "sleep_min": 1,
            "sleep_max": 2
        },
        "data_validation": {
            "min_data_points": 26,
            "max_consecutive_missing_days": 5,
            "net_value_min": 0.01,
            "net_value_max": 1000.0,
            "date_format": "%Y-%m-%d"
        },
        "technical_indicators": {
            "macd_fast": 12,
            "macd_slow": 26,
            "macd_signal": 9,
            "bollinger_window": 20,
            "bollinger_std": 2,
            "rsi_window": 14,
            "ma_window": 50,
            "rsi_oversold": 30,
            "rsi_overbought": 70,
            "ma_ratio_high": 1.2,
            "ma_ratio_low": 0.8
        },
        "trading_signals": {
            "stop_loss_percent": 0.10,
            "rsi_buy_threshold": 45,
            "rsi_sell_threshold": 65,
            "ma_ratio_buy_threshold": 1.0,
            "ma_ratio_sell_threshold": 1.2,
            "rsi_buy_strong": 35,
            "rsi_sell_weak": 65,
            "ma_ratio_strong_sell": 1.2,
            "ma_ratio_strong_buy": 0.9
        },
        "backtest": {
            "min_backtest_data": 100,
            "risk_free_rate": 0.03,
            "trading_days_per_year": 252
        },
        "portfolio": {
            "max_positions": 5,
            "suggested_allocation_base": 100
        }
    }
    
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                config = json.load(f)
            # åˆå¹¶é»˜è®¤é…ç½®å’Œç°æœ‰é…ç½®ï¼ˆç°æœ‰é…ç½®ä¼˜å…ˆï¼‰
            merged_config = {}
            for section, default_values in default_config.items():
                merged_config[section] = default_values.copy()
                if section in config:
                    merged_config[section].update(config[section])
            logger.info("é…ç½®æ–‡ä»¶ %s åŠ è½½æˆåŠŸ", CONFIG_FILE)
            return merged_config
        except Exception as e:
            logger.warning("åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: %sï¼Œä½¿ç”¨é»˜è®¤é…ç½®", e)
    else:
        try:
            with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, indent=2, ensure_ascii=False)
            logger.info("å·²åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶ %s", CONFIG_FILE)
        except Exception as e:
            logger.warning("åˆ›å»ºé…ç½®æ–‡ä»¶å¤±è´¥: %s", e)
    
    return default_config

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('market_monitor.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# åŠ è½½é…ç½®
CONFIG = load_config()

# å®šä¹‰æœ¬åœ°æ•°æ®å­˜å‚¨ç›®å½•
FUND_DATA_DIR = 'fund_data'
INDEX_DATA_DIR = 'index_data'
if not os.path.exists(FUND_DATA_DIR):
    os.makedirs(FUND_DATA_DIR)
if not os.path.exists(INDEX_DATA_DIR):
    os.makedirs(INDEX_DATA_DIR)

class MarketMonitor:
    def __init__(self, report_file='analysis_report.md', output_file='market_monitor_report.md', backtest_output_file='backtest_report.md'):
        self.report_file = report_file
        self.output_file = output_file
        self.backtest_output_file = backtest_output_file
        self.portfolio_output_file = 'portfolio_recommendation.md'
        self.fund_codes = []
        self.fund_data = {}
        self.index_code = '000300'  # æ²ªæ·±300æŒ‡æ•°ä»£ç 
        self.index_data = {}
        self.index_df = pd.DataFrame()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36'
        }
        # ä»é…ç½®ä¸­åŠ è½½å‚æ•°
        self.max_workers = CONFIG['network']['max_workers']
        self.retry_attempts = CONFIG['network']['retry_attempts']
        self.retry_wait_seconds = CONFIG['network']['retry_wait_seconds']
        self.request_timeout = CONFIG['network']['request_timeout']
        self.sleep_min = CONFIG['network']['sleep_min']
        self.sleep_max = CONFIG['network']['sleep_max']
        self.min_data_points = CONFIG['data_validation']['min_data_points']
        self.max_consecutive_missing_days = CONFIG['data_validation']['max_consecutive_missing_days']
        self.net_value_min = CONFIG['data_validation']['net_value_min']
        self.net_value_max = CONFIG['data_validation']['net_value_max']
        self.macd_fast = CONFIG['technical_indicators']['macd_fast']
        self.macd_slow = CONFIG['technical_indicators']['macd_slow']
        self.macd_signal = CONFIG['technical_indicators']['macd_signal']
        self.bollinger_window = CONFIG['technical_indicators']['bollinger_window']
        self.bollinger_std = CONFIG['technical_indicators']['bollinger_std']
        self.rsi_window = CONFIG['technical_indicators']['rsi_window']
        self.ma_window = CONFIG['technical_indicators']['ma_window']
        self.rsi_oversold = CONFIG['technical_indicators']['rsi_oversold']
        self.rsi_overbought = CONFIG['technical_indicators']['rsi_overbought']
        self.ma_ratio_high = CONFIG['technical_indicators']['ma_ratio_high']
        self.ma_ratio_low = CONFIG['technical_indicators']['ma_ratio_low']
        self.stop_loss_percent = CONFIG['trading_signals']['stop_loss_percent']
        self.rsi_buy_threshold = CONFIG['trading_signals']['rsi_buy_threshold']
        self.rsi_sell_threshold = CONFIG['trading_signals']['rsi_sell_threshold']
        self.ma_ratio_buy_threshold = CONFIG['trading_signals']['ma_ratio_buy_threshold']
        self.ma_ratio_sell_threshold = CONFIG['trading_signals']['ma_ratio_sell_threshold']
        self.rsi_buy_strong = CONFIG['trading_signals']['rsi_buy_strong']
        self.rsi_sell_weak = CONFIG['trading_signals']['rsi_sell_weak']
        self.ma_ratio_strong_sell = CONFIG['trading_signals']['ma_ratio_strong_sell']
        self.ma_ratio_strong_buy = CONFIG['trading_signals']['ma_ratio_strong_buy']
        self.min_backtest_data = CONFIG['backtest']['min_backtest_data']
        self.risk_free_rate = CONFIG['backtest']['risk_free_rate']
        self.trading_days_per_year = CONFIG['backtest']['trading_days_per_year']
        self.max_positions = CONFIG['portfolio']['max_positions']
        self.suggested_allocation_base = CONFIG['portfolio']['suggested_allocation_base']

    def _get_expected_latest_date(self):
        """æ ¹æ®å½“å‰æ—¶é—´ç¡®å®šæœŸæœ›çš„æœ€æ–°æ•°æ®æ—¥æœŸ"""
        now = datetime.now()
        # å‡è®¾å‡€å€¼æ›´æ–°æ—¶é—´ä¸ºæ™šä¸Š21:00
        update_time = time(21, 0)
        if now.time() < update_time:
            # å¦‚æœå½“å‰æ—¶é—´æ—©äº21:00ï¼Œåˆ™æœŸæœ›æœ€æ–°æ—¥æœŸä¸ºæ˜¨å¤©
            expected_date = now.date() - timedelta(days=1)
        else:
            # å¦åˆ™ï¼ŒæœŸæœ›æœ€æ–°æ—¥æœŸä¸ºä»Šå¤©
            expected_date = now.date()
        logger.info("å½“å‰æ—¶é—´: %s, æœŸæœ›æœ€æ–°æ•°æ®æ—¥æœŸ: %s", now.strftime('%Y-%m-%d %H:%M:%S'), expected_date)
        return expected_date

    def _validate_fund_data(self, df: pd.DataFrame, fund_code: str) -> Tuple[bool, str]:
        """
        éªŒè¯åŸºé‡‘æ•°æ®çš„å®Œæ•´æ€§å’Œåˆç†æ€§
        è¿”å› (æ˜¯å¦é€šè¿‡éªŒè¯, é”™è¯¯ä¿¡æ¯)
        """
        if df.empty:
            return False, f"åŸºé‡‘ {fund_code} æ•°æ®ä¸ºç©º"
        
        # 1. æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ['date', 'net_value']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            return False, f"åŸºé‡‘ {fund_code} ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}"
        
        # 2. æ£€æŸ¥æ•°æ®è¡Œæ•°
        if len(df) < self.min_data_points:
            return False, f"åŸºé‡‘ {fund_code} æ•°æ®è¡Œæ•°ä¸è¶³ ({len(df)} < {self.min_data_points})"
        
        # 3. æ£€æŸ¥æ—¥æœŸæ ¼å¼å’Œæ’åº
        try:
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
            invalid_dates = df[df['date'].isna()]
            if not invalid_dates.empty:
                return False, f"åŸºé‡‘ {fund_code} åŒ…å« {len(invalid_dates)} æ¡æ— æ•ˆæ—¥æœŸ"
            
            # æ£€æŸ¥æ—¥æœŸæ˜¯å¦è¿ç»­é€’å¢
            df_sorted = df.sort_values('date').reset_index(drop=True)
            date_diffs = df_sorted['date'].diff().dt.days
            consecutive_missing = (date_diffs > 1).sum()
            if consecutive_missing > self.max_consecutive_missing_days:
                logger.warning("åŸºé‡‘ %s å­˜åœ¨ %d å¤©ä»¥ä¸Šçš„æ—¥æœŸæ–­ç‚¹", fund_code, consecutive_missing)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤æ—¥æœŸ
            duplicates = df_sorted[df_sorted.duplicated(subset=['date'], keep=False)]
            if not duplicates.empty:
                logger.warning("åŸºé‡‘ %s å­˜åœ¨ %d æ¡é‡å¤æ—¥æœŸï¼Œå·²å»é‡", fund_code, len(duplicates))
                df_sorted = df_sorted.drop_duplicates(subset=['date'], keep='last')
        except Exception as e:
            return False, f"åŸºé‡‘ {fund_code} æ—¥æœŸè§£æå¤±è´¥: {e}"
        
        # 4. æ£€æŸ¥å‡€å€¼æ•°æ®
        try:
            df_sorted['net_value'] = pd.to_numeric(df_sorted['net_value'], errors='coerce')
            invalid_values = df_sorted[df_sorted['net_value'].isna()]
            if not invalid_values.empty:
                return False, f"åŸºé‡‘ {fund_code} åŒ…å« {len(invalid_values)} æ¡æ— æ•ˆå‡€å€¼"
            
            # æ£€æŸ¥å‡€å€¼èŒƒå›´
            out_of_range = df_sorted[
                (df_sorted['net_value'] < self.net_value_min) | 
                (df_sorted['net_value'] > self.net_value_max)
            ]
            if not out_of_range.empty:
                logger.warning("åŸºé‡‘ %s å­˜åœ¨ %d æ¡å‡€å€¼è¶…å‡ºèŒƒå›´çš„å€¼", fund_code, len(out_of_range))
            
            # æ£€æŸ¥å‡€å€¼æ˜¯å¦å•è°ƒé€’å¢
            net_value_diffs = df_sorted['net_value'].diff()
            negative_diffs = net_value_diffs[net_value_diffs < 0]
            if not negative_diffs.empty:
                logger.warning("åŸºé‡‘ %s å­˜åœ¨ %d æ¬¡å‡€å€¼è´Ÿå¢é•¿", fund_code, len(negative_diffs))
                
        except Exception as e:
            return False, f"åŸºé‡‘ {fund_code} å‡€å€¼è§£æå¤±è´¥: {e}"
        
        # 5. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        total_days = (df_sorted['date'].max() - df_sorted['date'].min()).days
        data_coverage = len(df_sorted) / (total_days + 1) * 100
        if data_coverage < 70:  # æ•°æ®è¦†ç›–ç‡ä½äº70%æ—¶è­¦å‘Š
            logger.warning("åŸºé‡‘ %s æ•°æ®è¦†ç›–ç‡è¾ƒä½: %.1f%%", fund_code, data_coverage)
        
        logger.info("åŸºé‡‘ %s æ•°æ®éªŒè¯é€šè¿‡: %d è¡Œæ•°æ®, è¦†ç›–ç‡ %.1f%%", 
                   fund_code, len(df_sorted), data_coverage)
        return True, ""

    def _parse_report(self, report_path='analysis_report.md'):
        """ä» analysis_report.md æå–æ¨èåŸºé‡‘ä»£ç """
        logger.info("æ­£åœ¨è§£æ %s è·å–æ¨èåŸºé‡‘ä»£ç ...", report_path)
        if not os.path.exists(report_path):
            logger.error("æŠ¥å‘Šæ–‡ä»¶ %s ä¸å­˜åœ¨", report_path)
            raise FileNotFoundError(f"{report_path} ä¸å­˜åœ¨")
        
        try:
            with open(report_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            pattern = re.compile(r'(?:^\| +(\d{6})|### åŸºé‡‘ (\d{6}))', re.M)
            matches = pattern.findall(content)

            extracted_codes = set()
            for match in matches:
                code = match[0] if match[0] else match[1]
                extracted_codes.add(code)
            
            sorted_codes = sorted(list(extracted_codes))
            self.fund_codes = sorted_codes[:1000]
            
            if not self.fund_codes:
                logger.warning("æœªæå–åˆ°ä»»ä½•æœ‰æ•ˆåŸºé‡‘ä»£ç ï¼Œè¯·æ£€æŸ¥ analysis_report.md")
            else:
                logger.info("æå–åˆ° %d ä¸ªåŸºé‡‘ï¼ˆæµ‹è¯•é™åˆ¶å‰1000ä¸ªï¼‰: %s", len(self.fund_codes), self.fund_codes)
            
        except Exception as e:
            logger.error("è§£ææŠ¥å‘Šæ–‡ä»¶å¤±è´¥: %s", e)
            raise

    def _read_local_data(self, fund_code):
        """è¯»å–æœ¬åœ°æ–‡ä»¶ï¼Œå¦‚æœå­˜åœ¨åˆ™è¿”å›DataFrame"""
        file_path = os.path.join(FUND_DATA_DIR, f"{fund_code}.csv")
        if os.path.exists(file_path):
            try:
                df = pd.read_csv(file_path, parse_dates=['date'])
                # éªŒè¯æœ¬åœ°æ•°æ®
                is_valid, error_msg = self._validate_fund_data(df, fund_code)
                if is_valid:
                    df = df.sort_values(by='date', ascending=True).reset_index(drop=True)
                    logger.info("æœ¬åœ°å·²å­˜åœ¨åŸºé‡‘ %s æ•°æ®ï¼Œå…± %d è¡Œï¼Œæœ€æ–°æ—¥æœŸä¸º: %s", fund_code, len(df), df['date'].max().date())
                    return df
                else:
                    logger.warning("æœ¬åœ°åŸºé‡‘ %s æ•°æ®éªŒè¯å¤±è´¥: %sï¼Œåˆ é™¤æ— æ•ˆæ–‡ä»¶", fund_code, error_msg)
                    try:
                        os.remove(file_path)
                    except:
                        pass
                    return pd.DataFrame()
            except Exception as e:
                logger.warning("è¯»å–æœ¬åœ°æ–‡ä»¶ %s å¤±è´¥: %s", file_path, e)
                # åˆ é™¤æŸåçš„æ–‡ä»¶
                try:
                    os.remove(file_path)
                except:
                    pass
        return pd.DataFrame()

    def _save_to_local_file(self, fund_code, df):
        """å°†DataFrameä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œè¦†ç›–æ—§æ–‡ä»¶"""
        # ä¿å­˜å‰å†æ¬¡éªŒè¯æ•°æ®
        is_valid, error_msg = self._validate_fund_data(df, fund_code)
        if not is_valid:
            logger.error("ä¿å­˜å‰æ•°æ®éªŒè¯å¤±è´¥: %s", error_msg)
            return False
        
        file_path = os.path.join(FUND_DATA_DIR, f"{fund_code}.csv")
        try:
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            df.to_csv(file_path, index=False)
            logger.info("åŸºé‡‘ %s æ•°æ®å·²æˆåŠŸä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶: %s", fund_code, file_path)
            return True
        except Exception as e:
            logger.error("ä¿å­˜åŸºé‡‘ %s æ•°æ®å¤±è´¥: %s", fund_code, e)
            return False

    @tenacity.retry(
        stop=tenacity.stop_after_attempt(lambda: CONFIG['network']['retry_attempts']),
        wait=tenacity.wait_fixed(lambda: CONFIG['network']['retry_wait_seconds']),
        retry=tenacity.retry_if_exception_type((requests.exceptions.RequestException, ValueError)),
        before_sleep=lambda retry_state: logger.info(f"é‡è¯•åŸºé‡‘ {retry_state.args[0]}ï¼Œç¬¬ {retry_state.attempt_number} æ¬¡")
    )
    def _fetch_fund_data(self, fund_code, latest_local_date=None):
        """
        ä»ç½‘ç»œè·å–åŸºé‡‘æ•°æ®ï¼Œå®ç°çœŸæ­£çš„å¢é‡æ›´æ–°ã€‚
        å¦‚æœ latest_local_date ä¸ä¸ºç©ºï¼Œåˆ™åªè·å–å…¶ä¹‹åçš„æ•°æ®ã€‚
        """
        all_new_data = []
        page_index = 1
        has_new_data = False
        
        while True:
            url = f"http://fundf10.eastmoney.com/F10DataApi.aspx?type=lsjz&code={fund_code}&page={page_index}&per=20"
            logger.info("æ­£åœ¨è·å–åŸºé‡‘ %s çš„ç¬¬ %d é¡µæ•°æ®...", fund_code, page_index)
            
            try:
                response = requests.get(url, headers=self.headers, timeout=self.request_timeout)
                response.raise_for_status()
                
                content_match = re.search(r'content:"(.*?)"', response.text, re.S)
                pages_match = re.search(r'pages:(\d+)', response.text)
                
                if not content_match or not pages_match:
                    logger.error("åŸºé‡‘ %s APIè¿”å›å†…å®¹æ ¼å¼ä¸æ­£ç¡®ï¼Œå¯èƒ½å·²æ— æ•°æ®æˆ–æ¥å£å˜æ›´", fund_code)
                    break

                raw_content_html = content_match.group(1).replace('\\"', '"')
                total_pages = int(pages_match.group(1))
                
                tables = pd.read_html(StringIO(raw_content_html))
                
                if not tables:
                    logger.warning("åŸºé‡‘ %s åœ¨ç¬¬ %d é¡µæœªæ‰¾åˆ°æ•°æ®è¡¨æ ¼ï¼Œçˆ¬å–ç»“æŸ", fund_code, page_index)
                    break
                
                df_page = tables[0]
                df_page.columns = ['date', 'net_value', 'cumulative_net_value', 'daily_growth_rate', 'purchase_status', 'redemption_status', 'dividend']
                df_page = df_page[['date', 'net_value']].copy()
                df_page['date'] = pd.to_datetime(df_page['date'], errors='coerce')
                df_page['net_value'] = pd.to_numeric(df_page['net_value'], errors='coerce')
                df_page = df_page.dropna(subset=['date', 'net_value'])
                
                # éªŒè¯å•é¡µæ•°æ®
                if not df_page.empty:
                    is_valid, error_msg = self._validate_fund_data(df_page, f"{fund_code}_page{page_index}")
                    if not is_valid:
                        logger.warning("ç¬¬ %d é¡µæ•°æ®éªŒè¯å¤±è´¥: %s", page_index, error_msg)
                        break
                
                # å¦‚æœæ˜¯å¢é‡æ›´æ–°æ¨¡å¼ï¼Œæ£€æŸ¥æ˜¯å¦å·²è·å–åˆ°æœ¬åœ°æœ€æ–°æ•°æ®ä¹‹å‰çš„æ•°æ®
                if latest_local_date:
                    new_df_page = df_page[df_page['date'].dt.date > latest_local_date]
                    if new_df_page.empty:
                        # å¦‚æœå½“å‰é¡µæ²¡æœ‰æ–°æ•°æ®ï¼Œä¸”ä¹‹å‰å·²ç»å‘ç°è¿‡æ–°æ•°æ®ï¼Œåˆ™åœæ­¢çˆ¬å–
                        if has_new_data:
                            logger.info("åŸºé‡‘ %s å·²è·å–æ‰€æœ‰æ–°æ•°æ®ï¼Œçˆ¬å–ç»“æŸã€‚", fund_code)
                            break
                        # å¦‚æœå½“å‰é¡µæ²¡æœ‰æ–°æ•°æ®ï¼Œä¸”æ˜¯ç¬¬ä¸€é¡µï¼Œåˆ™è¯´æ˜æ²¡æœ‰æ–°æ•°æ®
                        elif page_index == 1:
                            logger.info("åŸºé‡‘ %s æ— æ–°æ•°æ®ï¼Œçˆ¬å–ç»“æŸã€‚", fund_code)
                            break
                    else:
                        has_new_data = True
                        all_new_data.append(new_df_page)
                        logger.info("ç¬¬ %d é¡µ: å‘ç° %d è¡Œæ–°æ•°æ®", page_index, len(new_df_page))
                else:
                    # å¦‚æœæ˜¯é¦–æ¬¡ä¸‹è½½ï¼Œåˆ™è·å–æ‰€æœ‰æ•°æ®
                    all_new_data.append(df_page)

                logger.info("åŸºé‡‘ %s æ€»é¡µæ•°: %d, å½“å‰é¡µ: %d, å½“å‰é¡µè¡Œæ•°: %d", fund_code, total_pages, page_index, len(df_page))
                
                # å¦‚æœæ˜¯å¢é‡æ›´æ–°æ¨¡å¼ï¼Œä¸”å½“å‰é¡µæ•°æ®æ¯”æœ€æ–°æ•°æ®æ—¥æœŸæ—©ï¼Œåˆ™ç»“æŸå¾ªç¯
                if latest_local_date and (df_page['date'].dt.date <= latest_local_date).any():
                    logger.info("åŸºé‡‘ %s å·²è¿½æº¯åˆ°æœ¬åœ°æ•°æ®ï¼Œå¢é‡çˆ¬å–ç»“æŸã€‚", fund_code)
                    break

                if page_index >= total_pages:
                    logger.info("åŸºé‡‘ %s å·²è·å–æ‰€æœ‰å†å²æ•°æ®ï¼Œå…± %d é¡µï¼Œçˆ¬å–ç»“æŸ", fund_code, total_pages)
                    break
                
                page_index += 1
                time_module.sleep(random.uniform(self.sleep_min, self.sleep_max))  # ä½¿ç”¨é…ç½®çš„sleepæ—¶é—´
                
            except requests.exceptions.RequestException as e:
                logger.error("åŸºé‡‘ %s APIè¯·æ±‚å¤±è´¥: %s", fund_code, str(e))
                raise
            except Exception as e:
                logger.error("åŸºé‡‘ %s APIæ•°æ®è§£æå¤±è´¥: %s", fund_code, str(e))
                raise

        # åˆå¹¶æ–°æ•°æ®å¹¶è¿”å›
        if all_new_data:
            new_combined_df = pd.concat(all_new_data, ignore_index=True)
            # éªŒè¯åˆå¹¶åçš„æ•°æ®
            is_valid, error_msg = self._validate_fund_data(new_combined_df, fund_code)
            if not is_valid:
                logger.error("åˆå¹¶åçš„æ–°æ•°æ®éªŒè¯å¤±è´¥: %s", error_msg)
                return pd.DataFrame()
            return new_combined_df[['date', 'net_value']]
        else:
            return pd.DataFrame()

    def _load_index_data_from_file(self):
        """
        ä»æœ¬åœ°æ–‡ä»¶åŠ è½½å¤§ç›˜æŒ‡æ•°æ•°æ®
        """
        file_path = os.path.join(INDEX_DATA_DIR, f"{self.index_code}.csv")
        logger.info("æ­£åœ¨ä»æœ¬åœ°æ–‡ä»¶ %s åŠ è½½å¤§ç›˜æŒ‡æ•°æ•°æ®...", file_path)
        if not os.path.exists(file_path):
            logger.error("æœ¬åœ°å¤§ç›˜æŒ‡æ•°æ–‡ä»¶ %s ä¸å­˜åœ¨ï¼Œè¯·è¿è¡Œ download_index_data.py ä¸‹è½½ã€‚", file_path)
            return pd.DataFrame()
        
        try:
            df = pd.read_csv(file_path, parse_dates=['date'])
            # éªŒè¯æŒ‡æ•°æ•°æ®
            is_valid, error_msg = self._validate_fund_data(df, self.index_code)
            if not is_valid:
                logger.error("å¤§ç›˜æŒ‡æ•°æ•°æ®éªŒè¯å¤±è´¥: %s", error_msg)
                return pd.DataFrame()
            
            df = df.sort_values(by='date', ascending=True).reset_index(drop=True)
            logger.info("å¤§ç›˜æŒ‡æ•° %s æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± %d è¡Œï¼Œæœ€æ–°æ—¥æœŸä¸º: %s", self.index_code, len(df), df['date'].max().date())
            return df
        except Exception as e:
            logger.error("åŠ è½½æœ¬åœ°å¤§ç›˜æŒ‡æ•°æ–‡ä»¶ %s å¤±è´¥: %s", file_path, e)
            return pd.DataFrame()

    def _calculate_indicators(self, df):
        """è®¡ç®—æŠ€æœ¯æŒ‡æ ‡å¹¶ç”Ÿæˆç»“æœå­—å…¸"""
        if df is None or df.empty or len(df) < self.min_data_points:
            return None

        df = df.sort_values(by='date', ascending=True)
        
        # MACD - ä½¿ç”¨é…ç½®å‚æ•°
        exp_fast = df['net_value'].ewm(span=self.macd_fast, adjust=False).mean()
        exp_slow = df['net_value'].ewm(span=self.macd_slow, adjust=False).mean()
        df['macd'] = exp_fast - exp_slow
        df['signal'] = df['macd'].ewm(span=self.macd_signal, adjust=False).mean()

        # å¸ƒæ—å¸¦ - ä½¿ç”¨é…ç½®å‚æ•°
        df['bb_mid'] = df['net_value'].rolling(window=self.bollinger_window, min_periods=1).mean()
        df['bb_std'] = df['net_value'].rolling(window=self.bollinger_window, min_periods=1).std()
        df['bb_upper'] = df['bb_mid'] + (df['bb_std'] * self.bollinger_std)
        df['bb_lower'] = df['bb_mid'] - (df['bb_std'] * self.bollinger_std)
        
        # RSI - ä½¿ç”¨é…ç½®å‚æ•°
        delta = df['net_value'].diff()
        gain = delta.where(delta > 0, 0)
        loss = -delta.where(delta < 0, 0)
        
        avg_gain = gain.rolling(window=self.rsi_window, min_periods=1).mean()
        avg_loss = loss.rolling(window=self.rsi_window, min_periods=1).mean()
        
        rs = avg_gain / avg_loss.replace(0, np.nan)
        df['rsi'] = 100 - (100 / (1 + rs))

        # MA - ä½¿ç”¨é…ç½®å‚æ•°
        df['ma50'] = df['net_value'].rolling(window=min(self.ma_window, len(df)), min_periods=1).mean()
        df['ma_ratio'] = df['net_value'] / df['ma50']

        return df

    def _get_latest_signals(self, fund_code, df):
        """æ ¹æ®æœ€æ–°æ•°æ®è®¡ç®—ä¿¡å·"""
        try:
            processed_df = self._calculate_indicators(df)
            if processed_df is None:
                logger.warning("åŸºé‡‘ %s æ•°æ®ä¸è¶³ï¼Œè·³è¿‡è®¡ç®—", fund_code)
                return {
                    'fund_code': fund_code, 'latest_net_value': "æ•°æ®è·å–å¤±è´¥", 'rsi': np.nan, 'ma_ratio': np.nan,
                    'macd_diff': np.nan, 'bb_upper': np.nan, 'bb_lower': np.nan, 'bb_position': 'N/A', 'advice': "è§‚å¯Ÿ", 'action_signal': 'N/A'
                }
            
            latest_data = processed_df.iloc[-1]
            latest_net_value = latest_data['net_value']
            latest_rsi = latest_data['rsi']
            latest_ma50_ratio = latest_data['ma_ratio']
            latest_macd_diff = latest_data['macd'] - latest_data['signal']
            latest_bb_upper = latest_data['bb_upper']
            latest_bb_lower = latest_data['bb_lower']

            advice = "è§‚å¯Ÿ"
            if (not np.isnan(latest_rsi) and latest_rsi > self.rsi_overbought) or \
               (not np.isnan(latest_bb_upper) and latest_net_value > latest_bb_upper) or \
               (not np.isnan(latest_ma50_ratio) and latest_ma50_ratio > self.ma_ratio_high):
                advice = "ç­‰å¾…å›è°ƒ"
            elif (not np.isnan(latest_rsi) and latest_rsi < self.rsi_oversold) or \
                 (not np.isnan(latest_bb_lower) and latest_net_value < latest_bb_lower) or \
                 (not np.isnan(latest_ma50_ratio) and latest_ma50_ratio < self.ma_ratio_low):
                advice = "å¯åˆ†æ‰¹ä¹°å…¥"
            elif (not np.isnan(latest_ma50_ratio) and latest_ma50_ratio > 1) and \
                 (not np.isnan(latest_macd_diff) and latest_macd_diff > 0):
                advice = "å¯åˆ†æ‰¹ä¹°å…¥"
            elif (not np.isnan(latest_ma50_ratio) and latest_ma50_ratio < 1) and \
                 (not np.isnan(latest_macd_diff) and latest_macd_diff < 0):
                advice = "ç­‰å¾…å›è°ƒ"

            action_signal = "æŒæœ‰/è§‚å¯Ÿ"
            if not np.isnan(latest_ma50_ratio) and latest_ma50_ratio < 0.95:
                action_signal = "å¼ºå–å‡º/è§„é¿"
            elif (not np.isnan(latest_rsi) and latest_rsi > self.rsi_overbought) and \
                 (not np.isnan(latest_ma50_ratio) and latest_ma50_ratio > self.ma_ratio_strong_sell) and \
                 (not np.isnan(latest_macd_diff) and latest_macd_diff < 0):
                action_signal = "å¼ºå–å‡º/è§„é¿"
            elif (not np.isnan(latest_rsi) and latest_rsi > self.rsi_sell_weak) or \
                 (not np.isnan(latest_bb_upper) and latest_net_value > latest_bb_upper) or \
                 (not np.isnan(latest_ma50_ratio) and latest_ma50_ratio > self.ma_ratio_strong_sell):
                action_signal = "å¼±å–å‡º/è§„é¿"
            elif (not np.isnan(latest_rsi) and latest_rsi < self.rsi_buy_strong) and \
                 (not np.isnan(latest_ma50_ratio) and latest_ma50_ratio < self.ma_ratio_strong_buy) and \
                 (not np.isnan(latest_macd_diff) and latest_macd_diff > 0):
                action_signal = "å¼ºä¹°å…¥"
            elif (not np.isnan(latest_rsi) and latest_rsi < self.rsi_buy_threshold) or \
                 (not np.isnan(latest_bb_lower) and latest_net_value < latest_bb_lower) or \
                 (not np.isnan(latest_ma50_ratio) and latest_ma50_ratio < self.ma_ratio_buy_threshold):
                action_signal = "å¼±ä¹°å…¥"

            # è®¡ç®—å¸ƒæ—å¸¦ä½ç½®
            bb_position = "ä¸­è½¨"
            if not np.isnan(latest_net_value) and not np.isnan(latest_bb_upper) and latest_net_value > latest_bb_upper:
                bb_position = "ä¸Šè½¨ä¸Šæ–¹"
            elif not np.isnan(latest_net_value) and not np.isnan(latest_bb_lower) and latest_net_value < latest_bb_lower:
                bb_position = "ä¸‹è½¨ä¸‹æ–¹"

            return {
                'fund_code': fund_code,
                'latest_net_value': latest_net_value,
                'rsi': latest_rsi,
                'ma_ratio': latest_ma50_ratio,
                'macd_diff': latest_macd_diff,
                'bb_upper': latest_bb_upper,
                'bb_lower': latest_bb_lower,
                'bb_position': bb_position,
                'advice': advice,
                'action_signal': action_signal
            }
        except Exception as e:
            logger.error("å¤„ç†åŸºé‡‘ %s æ—¶å‘ç”Ÿå¼‚å¸¸: %s", fund_code, str(e))
            return {
                'fund_code': fund_code,
                'latest_net_value': "æ•°æ®è·å–å¤±è´¥",
                'rsi': np.nan,
                'ma_ratio': np.nan,
                'macd_diff': np.nan,
                'bb_upper': np.nan,
                'bb_lower': np.nan,
                'bb_position': 'N/A',
                'advice': "è§‚å¯Ÿ",
                'action_signal': 'N/A'
            }

    def _analyze_index(self, index_df):
        """åˆ†æå¤§ç›˜æŒ‡æ•°ï¼Œè®¡ç®—æŠ€æœ¯æŒ‡æ ‡å’Œä¿¡å·"""
        logger.info("å¼€å§‹åˆ†æå¤§ç›˜æŒ‡æ•° %s...", self.index_code)
        if not index_df.empty:
            self.index_df = index_df
            result = self._get_latest_signals(self.index_code, self.index_df.tail(100))
            self.index_data = result
            logger.info("å¤§ç›˜æŒ‡æ•° %s åˆ†æå®Œæˆ", self.index_code)
        else:
            logger.error("å¤§ç›˜æŒ‡æ•° %s æœªè·å–åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®", self.index_code)
            self.index_data = {
                'fund_code': self.index_code,
                'latest_net_value': "æ•°æ®è·å–å¤±è´¥",
                'rsi': np.nan,
                'ma_ratio': np.nan,
                'macd_diff': np.nan,
                'bb_upper': np.nan,
                'bb_lower': np.nan,
                'bb_position': 'N/A',
                'advice': "è§‚å¯Ÿ",
                'action_signal': 'N/A'
            }

    def _get_portfolio_signals(self, fund_data, max_positions=None):
        """
        æ ¹æ®ç»¼åˆè¯„åˆ†ç­›é€‰å‡ºå€¼å¾—ä¹°å…¥çš„åŸºé‡‘
        """
        if max_positions is None:
            max_positions = self.max_positions
            
        buy_signals = []
        for code, data in fund_data.items():
            if data['action_signal'] in ["å¼ºä¹°å…¥", "å¼±ä¹°å…¥"] and not np.isnan(data['rsi']):
                score = self._calculate_buy_score(data)
                buy_signals.append({
                    'code': code,
                    'signal': data['action_signal'],
                    'score': score,
                    'rsi': data['rsi'],
                    'ma_ratio': data['ma_ratio']
                })
        
        # æŒ‰ç…§è¯„åˆ†é™åºæ’åˆ—ï¼Œå–å‰Nä¸ª
        buy_signals = sorted(buy_signals, key=lambda x: x['score'], reverse=True)
        
        return buy_signals[:max_positions]

    def _calculate_buy_score(self, data):
        """
        è®¡ç®—åŸºé‡‘ä¹°å…¥è¯„åˆ†
        RSIè¶Šä½åˆ†æ•°è¶Šé«˜ï¼ŒMA_Ratioè¶Šä½åˆ†æ•°è¶Šé«˜ï¼Œå¸ƒæ—å¸¦ä½ç½®è¶Šä½åˆ†æ•°è¶Šé«˜
        """
        score = 0
        
        # 1. RSIè¯„åˆ†: 40åˆ† - ä½¿ç”¨é…ç½®çš„é˜ˆå€¼
        if data['rsi'] < self.rsi_oversold:
            score += 40
        elif data['rsi'] < self.rsi_buy_strong:
            score += 30
        elif data['rsi'] < self.rsi_buy_threshold:
            score += 20
        
        # 2. MA_Ratioè¯„åˆ†: 40åˆ† - ä½¿ç”¨é…ç½®çš„é˜ˆå€¼
        if data['ma_ratio'] < self.ma_ratio_strong_buy:
            score += 40
        elif data['ma_ratio'] < self.ma_ratio_low:
            score += 30
        elif data['ma_ratio'] < self.ma_ratio_buy_threshold:
            score += 20
        elif data['ma_ratio'] < 1.05:
            score += 10

        # 3. å¸ƒæ—å¸¦ä½ç½®è¯„åˆ†: 20åˆ†
        bb_position = data['bb_position']
        if bb_position == "ä¸‹è½¨ä¸‹æ–¹":
            score += 20
        elif bb_position == "ä¸­è½¨":
            score += 10
        else:
            score += 5
        
        return score

    def generate_portfolio_recommendation(self):
        """ç”ŸæˆæŠ•èµ„ç»„åˆæ¨è"""
        buy_candidates = self._get_portfolio_signals(self.fund_data, max_positions=3)
        
        with open(self.portfolio_output_file, 'w', encoding='utf-8') as f:
            f.write(f"# æŠ•èµ„ç»„åˆæ¨èæŠ¥å‘Š ({datetime.now().strftime('%Y-%m-%d')})\n\n")
            f.write("---")
            f.write(f"### å¤§ç›˜æŒ‡æ•° {self.index_code} å¸‚åœºæƒ…ç»ª\n\n")
            f.write(f"ğŸ“ˆ **æœ€æ–°å‡€å€¼**: {float(self.index_data['latest_net_value']):.2f}\n")
            f.write(f"ğŸ“Š **RSI**: {self.index_data['rsi']:.2f}\n")
            f.write(f"ğŸ“‰ **MA_Ratio**: {self.index_data['ma_ratio']:.2f}\n")
            f.write(f"ğŸ’¡ **å½“å‰ä¿¡å·**: {self.index_data['action_signal']} | {self.index_data['advice']}\n")
            f.write("---")
            f.write("\n## æ¨èåŸºé‡‘åˆ—è¡¨\n\n")
            
            if buy_candidates:
                f.write("| åºå· | ä¿¡å· | åŸºé‡‘ä»£ç  | è¯„åˆ† | RSI | MA_Ratio |\n")
                f.write("|------|------|----------|------|-----|----------|\n")
                for i, candidate in enumerate(buy_candidates, 1):
                    signal_emoji = "ğŸŸ¢ å¼ºä¹°å…¥" if candidate['signal'] == "å¼ºä¹°å…¥" else "ğŸŸ¡ å¼±ä¹°å…¥"
                    f.write(f"| {i} | {signal_emoji} | {candidate['code']} | {candidate['score']:.0f} | {candidate['rsi']:.1f} | {candidate['ma_ratio']:.2f} |\n")
                
                if buy_candidates:
                    suggested_amount = buy_candidates[0]['score'] // 10 * self.suggested_allocation_base
                    f.write(f"\n## å»ºè®®åˆ†é…\n")
                    f.write(f"ğŸ’° å»ºè®®æ¯æ”¯åŸºé‡‘åˆ†é…: {suggested_amount} å…ƒ\n\n")
                    f.write(f"ğŸ“ˆ ä»Šæ—¥ä¹°å…¥æœºä¼š: {len(buy_candidates)} / {len(self.fund_codes)}\n\n")
            else:
                f.write("## æ¨èç»“æœ\n")
                f.write("âŒ ä»Šæ—¥æ— ç¬¦åˆæ¡ä»¶çš„ä¹°å…¥æœºä¼šï¼Œå»ºè®®è§‚æœ›\n\n")
                f.write(f"ğŸ“Š æ€»æ‰«æåŸºé‡‘æ•°: {len(self.fund_codes)}\n\n")
        
        logger.info("æŠ•èµ„ç»„åˆæ¨èæŠ¥å‘Šç”Ÿæˆå®Œæˆ: %s", self.portfolio_output_file)

    def generate_detailed_report(self):
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        logger.info("æ­£åœ¨ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š: %s", self.output_file)
        with open(self.output_file, 'w', encoding='utf-8') as f:
            f.write(f"# å¸‚åœºç›‘æ§æŠ¥å‘Š ({datetime.now().strftime('%Y-%m-%d')})\n\n")
            f.write("---")
            f.write(f"### å¤§ç›˜æŒ‡æ•° {self.index_code} å¸‚åœºæƒ…ç»ª\n\n")
            f.write(f"ğŸ“ˆ **æœ€æ–°å‡€å€¼**: {self.index_data['latest_net_value']:.2f}\n")
            f.write(f"ğŸ“Š **RSI**: {self.index_data['rsi']:.2f}\n")
            f.write(f"ğŸ“‰ **MA_Ratio**: {self.index_data['ma_ratio']:.2f}\n")
            f.write(f"ğŸ’¡ **å½“å‰ä¿¡å·**: {self.index_data['action_signal']} | {self.index_data['advice']}\n")
            f.write("---")
            f.write("\n## åŸºé‡‘æ•°æ®åˆ†æ\n\n")
            f.write("| åŸºé‡‘ä»£ç  | æœ€æ–°å‡€å€¼ | RSI | MA50æ¯”ä¾‹ | MACDä¿¡å· | å¸ƒæ—å¸¦ä½ç½® | å»ºè®® | æ“ä½œä¿¡å· |\n")
            f.write("|----------|----------|-----|----------|----------|------------|------|----------|\n")
            
            for code, data in self.fund_data.items():
                line = (
                    f"| {data['fund_code']} | {data['latest_net_value']:.4f} | {data['rsi']:.2f} | {data['ma_ratio']:.2f} | "
                    f"{'é‡‘å‰' if data['macd_diff'] > 0 else 'æ­»å‰' if data['macd_diff'] < 0 else 'æ— ä¿¡å·'} | "
                    f"{data['bb_position']} | {data['advice']} | {data['action_signal']} |\n"
                )
                f.write(line)
        
        logger.info("è¯¦ç»†æŠ¥å‘Šç”Ÿæˆå®Œæˆ: %s", self.output_file)

    def generate_backtest_report(self):
        """ç”Ÿæˆå›æµ‹æŠ¥å‘Š"""
        logger.info("å¼€å§‹ç”Ÿæˆå›æµ‹æŠ¥å‘Š: %s", self.backtest_output_file)
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_code = {executor.submit(self._run_backtest_for_fund, code): code for code in self.fund_codes}
            for future in concurrent.futures.as_completed(future_to_code):
                fund_code = future_to_code[future]
                try:
                    result = future.result()
                    if result:
                        results.append(result)
                except Exception as e:
                    logger.error("å›æµ‹åŸºé‡‘ %s æ—¶å‘ç”Ÿå¼‚å¸¸: %s", fund_code, e)
        
        if not results:
            logger.warning("æ²¡æœ‰å¯ç”¨äºå›æµ‹çš„åŸºé‡‘æ•°æ®ã€‚")
            with open(self.backtest_output_file, 'w', encoding='utf-8') as f:
                f.write("# å†å²å›æµ‹æŠ¥å‘Š\n\n")
                f.write("---")
                f.write("\n\nâŒ æ²¡æœ‰å¯ç”¨äºå›æµ‹çš„åŸºé‡‘æ•°æ®ã€‚\n")
            return
        
        df_results = pd.DataFrame(results)
        df_results.sort_values(by='cagr', ascending=False, inplace=True)
        
        # ä¿å­˜è¯¦ç»†çš„å›æµ‹ç»“æœåˆ°CSV
        df_results.to_csv('backtest_results.csv', index=False, float_format='%.4f')

        with open(self.backtest_output_file, 'w', encoding='utf-8') as f:
            f.write("# å†å²å›æµ‹æŠ¥å‘Š\n\n")
            f.write("---")
            f.write("\n\n## ç»¼åˆè¡¨ç°æ’å (æŒ‰å¹´åŒ–æ”¶ç›Šç‡)\n\n")
            f.write(df_results.to_markdown(index=False, floatfmt=".2f"))
        
        logger.info("å›æµ‹æŠ¥å‘Šç”Ÿæˆå®Œæˆ: %s", self.backtest_output_file)
    
    def _run_backtest_for_fund(self, fund_code):
        df = self._read_local_data(fund_code)
        if df.empty or len(df) < self.min_backtest_data:
            logger.warning(f"åŸºé‡‘ {fund_code} æ•°æ®ä¸è¶³ï¼Œæ— æ³•å›æµ‹ã€‚")
            return None
        
        backtest_result = self._backtest_strategy(fund_code, df)
        backtest_result['fund_code'] = fund_code
        logger.info(f"åŸºé‡‘ {fund_code} å›æµ‹ç»“æœ: ç´¯è®¡å›æŠ¥={backtest_result['cum_return']:.2f}, æœ€å¤§å›æ’¤={backtest_result['max_drawdown']:.2f}, å¤æ™®æ¯”ç‡={backtest_result['sharpe_ratio']:.2f}, èƒœç‡={backtest_result['win_rate']:.2f}, å¹´åŒ–æ”¶ç›Šç‡={backtest_result['cagr']:.2f}, äº¤æ˜“æ¬¡æ•°={backtest_result['total_trades']}")
        
        return backtest_result

    def _backtest_strategy(self, fund_code, df):
        """å†å²å›æµ‹ç­–ç•¥æ€§èƒ½"""
        if df is None or df.empty or len(df) < self.min_backtest_data:
            logger.warning("åŸºé‡‘ %s æ•°æ®ä¸è¶³ï¼Œæ— æ³•å›æµ‹", fund_code)
            return {"cum_return": np.nan, "max_drawdown": np.nan, "sharpe_ratio": np.nan, "win_rate": np.nan, "cagr": np.nan, "total_trades": 0}

        # è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
        df = self._calculate_indicators(df)
        df = df.dropna()
        # å…³é”®ä¿®å¤ï¼šé‡ç½®ç´¢å¼•ä»¥ç¡®ä¿åç»­å¾ªç¯çš„.ilocæ­£å¸¸å·¥ä½œ
        df.reset_index(drop=True, inplace=True)

        # å¢åŠ ä¸€ä¸ªæ£€æŸ¥ï¼Œç¡®ä¿dropnaåä»æœ‰è¶³å¤Ÿçš„æ•°æ®
        if df.empty or len(df) < 2:
            logger.warning("åŸºé‡‘ %s è®¡ç®—æŒ‡æ ‡åæ•°æ®ä¸è¶³ï¼Œæ— æ³•å›æµ‹", fund_code)
            return {"cum_return": np.nan, "max_drawdown": np.nan, "sharpe_ratio": np.nan, "win_rate": np.nan, "cagr": np.nan, "total_trades": 0}

        # æ¨¡æ‹Ÿäº¤æ˜“
        position = 0
        buy_price = 0
        trades = []
        equity = [1.0] * len(df)
        for i in range(1, len(df)):
            latest_data = df.iloc[i]
            latest_net_value = latest_data['net_value']
            
            # æœ€å¤§å›æ’¤è®¡ç®—
            prev_net_value = df['net_value'].iloc[i-1]
            if prev_net_value != 0:
                equity[i] = equity[i-1] * (1 + (latest_net_value - prev_net_value) / prev_net_value)
            else:
                equity[i] = equity[i-1]

            # æ­¢æŸé€»è¾‘ - ä½¿ç”¨é…ç½®å‚æ•°
            if position == 1 and (latest_net_value / buy_price) < (1 - self.stop_loss_percent):
                sell_price = latest_net_value
                ret = (sell_price - buy_price) / buy_price
                trades.append({'buy_date': df.iloc[i-1]['date'], 'sell_date': df.iloc[i]['date'], 'return': ret, 'type': 'stop_loss'})
                position = 0
                buy_price = 0
                continue # ç»§ç»­ä¸‹ä¸€å¤©

            # äº¤æ˜“ä¿¡å·é€»è¾‘ - ä½¿ç”¨é…ç½®å‚æ•°
            latest_rsi = latest_data['rsi']
            latest_ma_ratio = latest_data['ma_ratio']
            latest_macd_diff = latest_data['macd'] - latest_data['signal']
            
            # ä¹°å…¥æ¡ä»¶ï¼šRSIä½äºé˜ˆå€¼æˆ–MA_Ratioä½äºé˜ˆå€¼ï¼Œä¸”MACDé‡‘å‰
            if position == 0 and \
               (latest_rsi < self.rsi_buy_threshold or latest_ma_ratio < self.ma_ratio_buy_threshold) and \
               (df.iloc[i-1]['macd'] - df.iloc[i-1]['signal'] <= 0 and latest_macd_diff > 0):
                
                position = 1
                buy_price = latest_net_value
                
            # å–å‡ºæ¡ä»¶ï¼šRSIé«˜äºé˜ˆå€¼æˆ–MA_Ratioé«˜äºé˜ˆå€¼ï¼Œä¸”MACDæ­»å‰
            elif position == 1 and \
                 (latest_rsi > self.rsi_sell_threshold or latest_ma_ratio > self.ma_ratio_sell_threshold) and \
                 (df.iloc[i-1]['macd'] - df.iloc[i-1]['signal'] >= 0 and latest_macd_diff < 0):
                
                sell_price = latest_net_value
                ret = (sell_price - buy_price) / buy_price
                trades.append({'buy_date': df.iloc[i-1]['date'], 'sell_date': df.iloc[i]['date'], 'return': ret, 'type': 'normal'})
                position = 0
                buy_price = 0
        
        # å¦‚æœå›æµ‹ç»“æŸæ—¶ä»æœ‰æŒä»“ï¼Œåˆ™ä»¥æœ€åä¸€å¤©å‡€å€¼æ¸…ä»“
        if position == 1:
            sell_price = df.iloc[-1]['net_value']
            ret = (sell_price - buy_price) / buy_price
            trades.append({'buy_date': buy_price, 'sell_date': sell_price, 'return': ret, 'type': 'final_sell'})

        # è®¡ç®—å›æµ‹æŒ‡æ ‡
        if not trades:
            return {"cum_return": np.nan, "max_drawdown": np.nan, "sharpe_ratio": np.nan, "win_rate": np.nan, "cagr": np.nan, "total_trades": 0}

        cum_return = np.product([1 + t['return'] for t in trades]) - 1
        
        equity_series = pd.Series(equity)
        max_drawdown = (equity_series / equity_series.cummax() - 1).min()
        
        win_trades = [t for t in trades if t['return'] > 0]
        win_rate = len(win_trades) / len(trades) if trades else 0
        
        daily_returns = df['net_value'].pct_change().dropna()
        if daily_returns.empty:
             sharpe_ratio = np.nan
             cagr = np.nan
        else:
            daily_risk_free = self.risk_free_rate / self.trading_days_per_year
            sharpe_ratio = (daily_returns.mean() - daily_risk_free) / daily_returns.std() * np.sqrt(self.trading_days_per_year)
            
            # è®¡ç®—å¹´åŒ–æ”¶ç›Šç‡ (CAGR)
            start_date = df['date'].iloc[0]
            end_date = df['date'].iloc[-1]
            total_years = (end_date - start_date).days / 365.25
            cagr = ((1 + cum_return) ** (1 / total_years)) - 1 if total_years > 0 else 0

        return {
            "cum_return": cum_return,
            "max_drawdown": max_drawdown,
            "sharpe_ratio": sharpe_ratio,
            "win_rate": win_rate,
            "cagr": cagr,
            "total_trades": len(trades)
        }

    def run(self):
        """ä¸»æ‰§è¡Œæµç¨‹"""
        try:
            # æ­¥éª¤ 1: åŠ è½½æœ¬åœ°æŒ‡æ•°æ•°æ®
            index_df = self._load_index_data_from_file()
            self._analyze_index(index_df)

            # æ­¥éª¤ 2: è§£ææ¨èåŸºé‡‘ä»£ç 
            self._parse_report()
            if not self.fund_codes:
                logger.error("æ²¡æœ‰æå–åˆ°ä»»ä½•åŸºé‡‘ä»£ç ï¼Œæ— æ³•ç»§ç»­å¤„ç†")
                return

            # æ­¥éª¤ 3: é¢„åŠ è½½æœ¬åœ°åŸºé‡‘æ•°æ®å¹¶æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸‹è½½
            logger.info("å¼€å§‹é¢„åŠ è½½æœ¬åœ°ç¼“å­˜æ•°æ®...")
            fund_codes_to_fetch = []
            expected_latest_date = self._get_expected_latest_date()
            for fund_code in self.fund_codes:
                local_df = self._read_local_data(fund_code)
                if not local_df.empty:
                    latest_local_date = local_df['date'].max().date()
                    data_points = len(local_df)
                    # æ£€æŸ¥æ•°æ®æ˜¯å¦æœ€æ–°ä¸”å®Œæ•´
                    if latest_local_date >= expected_latest_date and data_points >= self.min_data_points:
                        logger.info("åŸºé‡‘ %s çš„æœ¬åœ°æ•°æ®å·²æ˜¯æœ€æ–° (%s, æœŸæœ›: %s) ä¸”æ•°æ®é‡è¶³å¤Ÿ (%d è¡Œ)ï¼Œç›´æ¥åŠ è½½ã€‚", fund_code, latest_local_date, expected_latest_date, data_points)
                        self.fund_data[fund_code] = self._get_latest_signals(fund_code, local_df.tail(100))
                        continue
                    else:
                        if latest_local_date < expected_latest_date:
                            logger.info("åŸºé‡‘ %s æœ¬åœ°æ•°æ®å·²è¿‡æ—¶ï¼ˆæœ€æ–°æ—¥æœŸä¸º %sï¼ŒæœŸæœ› %sï¼‰ï¼Œéœ€è¦ä»ç½‘ç»œè·å–æ–°æ•°æ®ã€‚", fund_code, latest_local_date, expected_latest_date)
                        if data_points < self.min_data_points:
                            logger.info("åŸºé‡‘ %s æœ¬åœ°æ•°æ®é‡ä¸è¶³ï¼ˆä»… %d è¡Œï¼Œéœ€è‡³å°‘ %d è¡Œï¼‰ï¼Œéœ€è¦ä»ç½‘ç»œè·å–ã€‚", fund_code, data_points, self.min_data_points)
                else:
                    logger.info("åŸºé‡‘ %s æœ¬åœ°æ•°æ®ä¸å­˜åœ¨ï¼Œéœ€è¦ä»ç½‘ç»œè·å–ã€‚", fund_code)
                fund_codes_to_fetch.append(fund_code)
            
            # æ­¥éª¤ 4: å¤šçº¿ç¨‹ç½‘ç»œä¸‹è½½å’Œå¤„ç†
            if fund_codes_to_fetch:
                logger.info("å¼€å§‹ä½¿ç”¨å¤šçº¿ç¨‹è·å– %d ä¸ªåŸºé‡‘çš„æ–°æ•°æ®...", len(fund_codes_to_fetch))
                with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    future_to_code = {executor.submit(self._process_single_fund, code): code for code in fund_codes_to_fetch}
                    for future in concurrent.futures.as_completed(future_to_code):
                        fund_code = future_to_code[future]
                        try:
                            result = future.result()
                            if result:
                                self.fund_data[fund_code] = result
                        except Exception as e:
                            logger.error("å¤„ç†åŸºé‡‘ %s æ•°æ®æ—¶å‡ºé”™: %s", fund_code, str(e))
                            self.fund_data[fund_code] = {
                                'fund_code': fund_code,
                                'latest_net_value': "æ•°æ®è·å–å¤±è´¥",
                                'rsi': np.nan,
                                'ma_ratio': np.nan,
                                'macd_diff': np.nan,
                                'bb_upper': np.nan,
                                'bb_lower': np.nan,
                                'bb_position': 'N/A',
                                'advice': "è§‚å¯Ÿ",
                                'action_signal': 'N/A'
                            }
            else:
                logger.info("æ‰€æœ‰åŸºé‡‘æ•°æ®å‡æ¥è‡ªæœ¬åœ°ç¼“å­˜ï¼Œæ— éœ€ç½‘ç»œä¸‹è½½ã€‚")

            if len(self.fund_data) > 0:
                logger.info("æ‰€æœ‰åŸºé‡‘æ•°æ®å¤„ç†å®Œæˆã€‚")
            else:
                logger.error("æ‰€æœ‰åŸºé‡‘æ•°æ®å‡è·å–å¤±è´¥ã€‚")
            
            # æ­¥éª¤ 5: ç”ŸæˆæŠ¥å‘Š
            self.generate_portfolio_recommendation()
            self.generate_detailed_report()
            self.generate_backtest_report()

        except Exception as e:
            logger.exception("è„šæœ¬æ‰§è¡Œå¤±è´¥: %s", e)

    def _process_single_fund(self, fund_code):
        """å¤„ç†å•ä¸ªåŸºé‡‘æ•°æ®ï¼šè¯»å–æœ¬åœ°ï¼Œä¸‹è½½å¢é‡ï¼Œåˆå¹¶ï¼Œä¿å­˜ï¼Œå¹¶è®¡ç®—ä¿¡å·"""
        local_df = self._read_local_data(fund_code)
        latest_local_date = local_df['date'].max().date() if not local_df.empty else None
        
        new_df = self._fetch_fund_data(fund_code, latest_local_date)
        
        if not new_df.empty:
            df_final = pd.concat([local_df, new_df]).drop_duplicates(subset=['date'], keep='last').sort_values(by='date', ascending=True)
            if self._save_to_local_file(fund_code, df_final):
                return self._get_latest_signals(fund_code, df_final.tail(100))
            else:
                logger.error("åŸºé‡‘ %s ä¿å­˜å¤±è´¥ï¼Œè·³è¿‡", fund_code)
                return None
        elif not local_df.empty:
            # å¦‚æœæ²¡æœ‰æ–°æ•°æ®ï¼Œä¸”æœ¬åœ°æœ‰æ•°æ®ï¼Œåˆ™ä½¿ç”¨æœ¬åœ°æ•°æ®è®¡ç®—ä¿¡å·
            logger.info("åŸºé‡‘ %s æ— æ–°æ•°æ®ï¼Œä½¿ç”¨æœ¬åœ°å†å²æ•°æ®è¿›è¡Œåˆ†æ", fund_code)
            return self._get_latest_signals(fund_code, local_df.tail(100))
        else:
            # å¦‚æœæ—¢æ²¡æœ‰æ–°æ•°æ®ï¼Œæœ¬åœ°åˆæ²¡æœ‰æ•°æ®ï¼Œåˆ™è¿”å›å¤±è´¥
            logger.error("åŸºé‡‘ %s æœªè·å–åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼Œä¸”æœ¬åœ°æ— ç¼“å­˜", fund_code)
            return None

if __name__ == "__main__":
    try:
        logger.info("è„šæœ¬å¯åŠ¨")
        monitor = MarketMonitor()
        monitor.run()
        logger.info("è„šæœ¬è¿è¡Œç»“æŸ")
    except Exception as e:
        logger.exception("è„šæœ¬è¿è¡Œå¤±è´¥: %s", e)
