import pandas as pd
import numpy as np
import re
import os
import logging
from datetime import datetime, timedelta, time
import random
from io import StringIO
import requests
import tenacity
import concurrent.futures
import time as time_module
import json
from typing import Dict, Optional, Tuple

# é…ç½®å¤–éƒ¨åŒ– - ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°
CONFIG_FILE = 'market_monitor_config.json'

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºé»˜è®¤é…ç½®"""
    default_config = {
        "network": {
            "max_workers": 5,
            "retry_attempts": 5,
            "retry_wait_seconds": 10,
            "request_timeout": 30,
            "sleep_min": 1,
            "sleep_max": 2
        },
        "data_validation": {
            "min_data_points": 26,
            "max_consecutive_missing_days": 5,
            "net_value_min": 0.01,
            "net_value_max": 1000.0,
            "date_format": "%Y-%m-%d"
        },
        "technical_indicators": {
            "macd_fast": 12,
            "macd_slow": 26,
            "macd_signal": 9,
            "bollinger_window": 20,
            "bollinger_std": 2,
            "rsi_window": 14,
            "ma_window": 50,
            "rsi_oversold": 30,
            "rsi_overbought": 70,
            "ma_ratio_high": 1.2,
            "ma_ratio_low": 0.8
        },
        "trading_signals": {
            "stop_loss_percent": 0.10,
            "rsi_buy_threshold": 45,
            "rsi_sell_threshold": 65,
            "ma_ratio_buy_threshold": 1.0,
            "ma_ratio_sell_threshold": 1.2,
            "rsi_buy_strong": 35,
            "rsi_sell_weak": 65,
            "ma_ratio_strong_sell": 1.2,
            "ma_ratio_strong_buy": 0.9
        },
        "backtest": {
            "min_backtest_data": 100,
            "risk_free_rate": 0.03,
            "trading_days_per_year": 252
        },
        "portfolio": {
            "max_positions": 5,
            "suggested_allocation_base": 100
        }
    }
    
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                config = json.load(f)
            # åˆå¹¶é»˜è®¤é…ç½®å’Œç°æœ‰é…ç½®ï¼ˆç°æœ‰é…ç½®ä¼˜å…ˆï¼‰
            merged_config = {}
            for section, default_values in default_config.items():
                merged_config[section] = default_values.copy()
                if section in config:
                    merged_config[section].update(config[section])
            logger.info("é…ç½®æ–‡ä»¶ %s åŠ è½½æˆåŠŸ", CONFIG_FILE)
            return merged_config
        except Exception as e:
            logger.warning("åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: %sï¼Œä½¿ç”¨é»˜è®¤é…ç½®", e)
    else:
        try:
            with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, indent=2, ensure_ascii=False)
            logger.info("å·²åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶ %s", CONFIG_FILE)
        except Exception as e:
            logger.warning("åˆ›å»ºé…ç½®æ–‡ä»¶å¤±è´¥: %s", e)
    
    return default_config

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('market_monitor.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# åŠ è½½é…ç½®
CONFIG = load_config()

# å®šä¹‰æœ¬åœ°æ•°æ®å­˜å‚¨ç›®å½•
FUND_DATA_DIR = 'fund_data'
INDEX_DATA_DIR = 'index_data'
if not os.path.exists(FUND_DATA_DIR):
    os.makedirs(FUND_DATA_DIR)
if not os.path.exists(INDEX_DATA_DIR):
    os.makedirs(INDEX_DATA_DIR)

class MarketMonitor:
    def __init__(self, report_file='analysis_report.md', output_file='market_monitor_report.md', backtest_output_file='backtest_report.md'):
        self.report_file = report_file
        self.output_file = output_file
        self.backtest_output_file = backtest_output_file
        self.portfolio_output_file = 'portfolio_recommendation.md'
        self.fund_codes = []
        self.fund_data = {}
        self.index_code = '000300'  # æ²ªæ·±300æŒ‡æ•°ä»£ç 
        self.index_data = {}
        self.index_df = pd.DataFrame()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36'
        }
        # ä»é…ç½®ä¸­åŠ è½½å‚æ•°
        self.max_workers = CONFIG['network']['max_workers']
        self.retry_attempts = CONFIG['network']['retry_attempts']
        self.retry_wait_seconds = CONFIG['network']['retry_wait_seconds']
        self.request_timeout = CONFIG['network']['request_timeout']
        self.sleep_min = CONFIG['network']['sleep_min']
        self.sleep_max = CONFIG['network']['sleep_max']
        self.min_data_points = CONFIG['data_validation']['min_data_points']
        self.max_consecutive_missing_days = CONFIG['data_validation']['max_consecutive_missing_days']
        self.net_value_min = CONFIG['data_validation']['net_value_min']
        self.net_value_max = CONFIG['data_validation']['net_value_max']
        self.macd_fast = CONFIG['technical_indicators']['macd_fast']
        self.macd_slow = CONFIG['technical_indicators']['macd_slow']
        self.macd_signal = CONFIG['technical_indicators']['macd_signal']
        self.bollinger_window = CONFIG['technical_indicators']['bollinger_window']
        self.bollinger_std = CONFIG['technical_indicators']['bollinger_std']
        self.rsi_window = CONFIG['technical_indicators']['rsi_window']
        self.ma_window = CONFIG['technical_indicators']['ma_window']
        self.rsi_oversold = CONFIG['technical_indicators']['rsi_oversold']
        self.rsi_overbought = CONFIG['technical_indicators']['rsi_overbought']
        self.ma_ratio_high = CONFIG['technical_indicators']['ma_ratio_high']
        self.ma_ratio_low = CONFIG['technical_indicators']['ma_ratio_low']
        self.stop_loss_percent = CONFIG['trading_signals']['stop_loss_percent']
        self.rsi_buy_threshold = CONFIG['trading_signals']['rsi_buy_threshold']
        self.rsi_sell_threshold = CONFIG['trading_signals']['rsi_sell_threshold']
        self.ma_ratio_buy_threshold = CONFIG['trading_signals']['ma_ratio_buy_threshold']
        self.ma_ratio_sell_threshold = CONFIG['trading_signals']['ma_ratio_sell_threshold']
        self.rsi_buy_strong = CONFIG['trading_signals']['rsi_buy_strong']
        self.rsi_sell_weak = CONFIG['trading_signals']['rsi_sell_weak']
        self.ma_ratio_strong_sell = CONFIG['trading_signals']['ma_ratio_strong_sell']
        self.ma_ratio_strong_buy = CONFIG['trading_signals']['ma_ratio_strong_buy']
        self.min_backtest_data = CONFIG['backtest']['min_backtest_data']
        self.risk_free_rate = CONFIG['backtest']['risk_free_rate']
        self.trading_days_per_year = CONFIG['backtest']['trading_days_per_year']
        self.max_positions = CONFIG['portfolio']['max_positions']
        self.suggested_allocation_base = CONFIG['portfolio']['suggested_allocation_base']

    def _get_expected_latest_date(self):
        """æ ¹æ®å½“å‰æ—¶é—´ç¡®å®šæœŸæœ›çš„æœ€æ–°æ•°æ®æ—¥æœŸ"""
        now = datetime.now()
        # å‡è®¾å‡€å€¼æ›´æ–°æ—¶é—´ä¸ºæ™šä¸Š21:00
        update_time = time(21, 0)
        if now.time() < update_time:
            # å¦‚æœå½“å‰æ—¶é—´æ—©äº21:00ï¼Œåˆ™æœŸæœ›æœ€æ–°æ—¥æœŸä¸ºæ˜¨å¤©
            expected_date = now.date() - timedelta(days=1)
        else:
            # å¦åˆ™ï¼ŒæœŸæœ›æœ€æ–°æ—¥æœŸä¸ºä»Šå¤©
            expected_date = now.date()
        logger.info("å½“å‰æ—¶é—´: %s, æœŸæœ›æœ€æ–°æ•°æ®æ—¥æœŸ: %s", now.strftime('%Y-%m-%d %H:%M:%S'), expected_date)
        return expected_date

    def _validate_fund_data(self, df: pd.DataFrame, fund_code: str) -> Tuple[bool, str]:
        """
        éªŒè¯åŸºé‡‘æ•°æ®çš„å®Œæ•´æ€§å’Œåˆç†æ€§
        è¿”å› (æ˜¯å¦é€šè¿‡éªŒè¯, é”™è¯¯ä¿¡æ¯)
        """
        if df.empty:
            return False, f"åŸºé‡‘ {fund_code} æ•°æ®ä¸ºç©º"
        
        # 1. æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ['date', 'net_value']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            return False, f"åŸºé‡‘ {fund_code} ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}"
        
        # 2. æ£€æŸ¥æ•°æ®è¡Œæ•°
        if len(df) < self.min_data_points:
            return False, f"åŸºé‡‘ {fund_code} æ•°æ®è¡Œæ•°ä¸è¶³ ({len(df)} < {self.min_data_points})"
        
        # 3. æ£€æŸ¥æ—¥æœŸæ ¼å¼å’Œæ’åº
        try:
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
            invalid_dates = df[df['date'].isna()]
            if not invalid_dates.empty:
                return False, f"åŸºé‡‘ {fund_code} åŒ…å« {len(invalid_dates)} æ¡æ— æ•ˆæ—¥æœŸ"
            
            # æ£€æŸ¥æ—¥æœŸæ˜¯å¦è¿ç»­é€’å¢
            df_sorted = df.sort_values('date').reset_index(drop=True)
            date_diffs = df_sorted['date'].diff().dt.days
            consecutive_missing = (date_diffs > 1).sum()
            if consecutive_missing > self.max_consecutive_missing_days:
                logger.warning("åŸºé‡‘ %s å­˜åœ¨ %d å¤©ä»¥ä¸Šçš„æ—¥æœŸæ–­ç‚¹", fund_code, consecutive_missing)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤æ—¥æœŸ
            duplicates = df_sorted[df_sorted.duplicated(subset=['date'], keep=False)]
            if not duplicates.empty:
                logger.warning("åŸºé‡‘ %s å­˜åœ¨ %d æ¡é‡å¤æ—¥æœŸï¼Œå·²å»é‡", fund_code, len(duplicates))
                df_sorted = df_sorted.drop_duplicates(subset=['date'], keep='last')
        except Exception as e:
            return False, f"åŸºé‡‘ {fund_code} æ—¥æœŸè§£æå¤±è´¥: {e}"
        
        # 4. æ£€æŸ¥å‡€å€¼æ•°æ®
        try:
            df_sorted['net_value'] = pd.to_numeric(df_sorted['net_value'], errors='coerce')
            invalid_values = df_sorted[df_sorted['net_value'].isna()]
            if not invalid_values.empty:
                return False, f"åŸºé‡‘ {fund_code} åŒ…å« {len(invalid_values)} æ¡æ— æ•ˆå‡€å€¼"
            
            # æ£€æŸ¥å‡€å€¼èŒƒå›´
            out_of_range = df_sorted[
                (df_sorted['net_value'] < self.net_value_min) |
                (df_sorted['net_value'] > self.net_value_max)
            ]
            if not out_of_range.empty:
                logger.warning("åŸºé‡‘ %s å­˜åœ¨ %d æ¡å‡€å€¼è¶…å‡ºèŒƒå›´çš„å€¼", fund_code, len(out_of_range))
            
            # æ£€æŸ¥å‡€å€¼æ˜¯å¦å•è°ƒé€’å¢
            net_value_diffs = df_sorted['net_value'].diff()
            negative_diffs = net_value_diffs[net_value_diffs < 0]
            if not negative_diffs.empty:
                logger.warning("åŸºé‡‘ %s å­˜åœ¨ %d æ¬¡å‡€å€¼è´Ÿå¢é•¿", fund_code, len(negative_diffs))
                
        except Exception as e:
            return False, f"åŸºé‡‘ {fund_code} å‡€å€¼è§£æå¤±è´¥: {e}"
        
        # 5. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        total_days = (df_sorted['date'].max() - df_sorted['date'].min()).days
        data_coverage = len(df_sorted) / (total_days + 1) * 100
        if data_coverage < 70:  # æ•°æ®è¦†ç›–ç‡ä½äº70%æ—¶è­¦å‘Š
            logger.warning("åŸºé‡‘ %s æ•°æ®è¦†ç›–ç‡è¾ƒä½: %.1f%%", fund_code, data_coverage)
        
        logger.info("åŸºé‡‘ %s æ•°æ®éªŒè¯é€šè¿‡: %d è¡Œæ•°æ®, è¦†ç›–ç‡ %.1f%%", 
                    fund_code, len(df_sorted), data_coverage)
        return True, ""

    def _parse_report(self, report_path='analysis_report.md'):
        """ä» analysis_report.md æå–æ¨èåŸºé‡‘ä»£ç """
        logger.info("æ­£åœ¨è§£æ %s è·å–æ¨èåŸºé‡‘ä»£ç ...", report_path)
        if not os.path.exists(report_path):
            logger.error("æŠ¥å‘Šæ–‡ä»¶ %s ä¸å­˜åœ¨", report_path)
            raise FileNotFoundError(f"{report_path} ä¸å­˜åœ¨")
        
        try:
            with open(report_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            pattern = re.compile(r'(?:^\| +(\d{6})|### åŸºé‡‘ (\d{6}))', re.M)
            matches = pattern.findall(content)

            extracted_codes = set()
            for match in matches:
                code = match[0] if match[0] else match[1]
                extracted_codes.add(code)
            
            sorted_codes = sorted(list(extracted_codes))
            self.fund_codes = sorted_codes[:1000]
            
            if not self.fund_codes:
                logger.warning("æœªæå–åˆ°ä»»ä½•æœ‰æ•ˆåŸºé‡‘ä»£ç ï¼Œè¯·æ£€æŸ¥ analysis_report.md")
            else:
                logger.info("æå–åˆ° %d ä¸ªåŸºé‡‘ï¼ˆæµ‹è¯•é™åˆ¶å‰1000ä¸ªï¼‰: %s", len(self.fund_codes), self.fund_codes)
            
        except Exception as e:
            logger.error("è§£ææŠ¥å‘Šæ–‡ä»¶å¤±è´¥: %s", e)
            raise

    def _read_local_data(self, fund_code):
        """è¯»å–æœ¬åœ°æ–‡ä»¶ï¼Œå¦‚æœå­˜åœ¨åˆ™è¿”å›DataFrame"""
        file_path = os.path.join(FUND_DATA_DIR, f"{fund_code}.csv")
        if os.path.exists(file_path):
            try:
                df = pd.read_csv(file_path, parse_dates=['date'])
                # éªŒè¯æœ¬åœ°æ•°æ®
                is_valid, error_msg = self._validate_fund_data(df, fund_code)
                if is_valid:
                    df = df.sort_values(by='date', ascending=True).reset_index(drop=True)
                    logger.info("æœ¬åœ°å·²å­˜åœ¨åŸºé‡‘ %s æ•°æ®ï¼Œå…± %d è¡Œï¼Œæœ€æ–°æ—¥æœŸä¸º: %s", fund_code, len(df), df['date'].max().date())
                    return df
                else:
                    logger.warning("æœ¬åœ°åŸºé‡‘ %s æ•°æ®éªŒè¯å¤±è´¥: %sï¼Œåˆ é™¤æ— æ•ˆæ–‡ä»¶", fund_code, error_msg)
                    try:
                        os.remove(file_path)
                    except:
                        pass
                    return pd.DataFrame()
            except Exception as e:
                logger.warning("è¯»å–æœ¬åœ°æ–‡ä»¶ %s å¤±è´¥: %s", file_path, e)
                # åˆ é™¤æŸåçš„æ–‡ä»¶
                try:
                    os.remove(file_path)
                except:
                    pass
        return pd.DataFrame()

    def _save_to_local_file(self, fund_code, df):
        """å°†DataFrameä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œè¦†ç›–æ—§æ–‡ä»¶"""
        # ä¿å­˜å‰å†æ¬¡éªŒè¯æ•°æ®
        is_valid, error_msg = self._validate_fund_data(df, fund_code)
        if not is_valid:
            logger.error("ä¿å­˜å‰æ•°æ®éªŒè¯å¤±è´¥: %s", error_msg)
            return False
        
        file_path = os.path.join(FUND_DATA_DIR, f"{fund_code}.csv")
        try:
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            df.to_csv(file_path, index=False)
            logger.info("åŸºé‡‘ %s æ•°æ®å·²æˆåŠŸä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶: %s", fund_code, file_path)
            return True
        except Exception as e:
            logger.error("ä¿å­˜åŸºé‡‘ %s æ•°æ®å¤±è´¥: %s", fund_code, e)
            return False

    @tenacity.retry(
        stop=tenacity.stop_after_attempt(lambda: CONFIG['network']['retry_attempts']),
        wait=tenacity.wait_fixed(lambda: CONFIG['network']['retry_wait_seconds']),
        retry=tenacity.retry_if_exception_type((requests.exceptions.RequestException, ValueError)),
        before_sleep=lambda retry_state: logger.info(f"é‡è¯•åŸºé‡‘ {retry_state.args[0]}ï¼Œç¬¬ {retry_state.attempt_number} æ¬¡")
    )
    def _fetch_fund_data(self, fund_code, latest_local_date=None):
        """
        ä»ç½‘ç»œè·å–åŸºé‡‘æ•°æ®ï¼Œå®ç°çœŸæ­£çš„å¢é‡æ›´æ–°ã€‚
        å¦‚æœ latest_local_date ä¸ä¸ºç©ºï¼Œåˆ™åªè·å–å…¶ä¹‹åçš„æ•°æ®ã€‚
        """
        all_new_data = []
        page_index = 1
        has_new_data = False
        
        while True:
            url = f"http://fundf10.eastmoney.com/F10DataApi.aspx?type=lsjz&code={fund_code}&page={page_index}&per=20"
            logger.info("æ­£åœ¨è·å–åŸºé‡‘ %s çš„ç¬¬ %d é¡µæ•°æ®...", fund_code, page_index)
            
            try:
                response = requests.get(url, headers=self.headers, timeout=self.request_timeout)
                response.raise_for_status()
                
                content_match = re.search(r'content:"(.*?)"', response.text, re.S)
                pages_match = re.search(r'pages:(\d+)', response.text)
                
                if not content_match or not pages_match:
                    logger.error("åŸºé‡‘ %s APIè¿”å›å†…å®¹æ ¼å¼ä¸æ­£ç¡®ï¼Œå¯èƒ½å·²æ— æ•°æ®æˆ–æ¥å£å˜æ›´", fund_code)
                    break

                raw_content_html = content_match.group(1).replace('\\"', '"')
                total_pages = int(pages_match.group(1))
                
                tables = pd.read_html(StringIO(raw_content_html))
                
                if not tables:
                    logger.warning("åŸºé‡‘ %s åœ¨ç¬¬ %d é¡µæœªæ‰¾åˆ°æ•°æ®è¡¨æ ¼ï¼Œçˆ¬å–ç»“æŸ", fund_code, page_index)
                    break
                
                df_page = tables[0]
                df_page.columns = ['date', 'net_value', 'cumulative_net_value', 'daily_growth_rate', 'purchase_status', 'redemption_status', 'dividend']
                df_page = df_page[['date', 'net_value']].copy()
                df_page['date'] = pd.to_datetime(df_page['date'], errors='coerce')
                df_page['net_value'] = pd.to_numeric(df_page['net_value'], errors='coerce')
                df_page = df_page.dropna(subset=['date', 'net_value'])
                
                # éªŒè¯å•é¡µæ•°æ®
                if not df_page.empty:
                    is_valid, error_msg = self._validate_fund_data(df_page, f"{fund_code}_page{page_index}")
                    if not is_valid:
                        logger.warning("ç¬¬ %d é¡µæ•°æ®éªŒè¯å¤±è´¥: %s", page_index, error_msg)
                        break
                
                # å¦‚æœæ˜¯å¢é‡æ›´æ–°æ¨¡å¼ï¼Œæ£€æŸ¥æ˜¯å¦å·²è·å–åˆ°æœ¬åœ°æœ€æ–°æ•°æ®ä¹‹å‰çš„æ•°æ®
                if latest_local_date:
                    new_df_page = df_page[df_page['date'].dt.date > latest_local_date]
                    if new_df_page.empty:
                        # å¦‚æœå½“å‰é¡µæ²¡æœ‰æ–°æ•°æ®ï¼Œä¸”ä¹‹å‰å·²ç»å‘ç°è¿‡æ–°æ•°æ®ï¼Œåˆ™åœæ­¢çˆ¬å–
                        if has_new_data:
                            logger.info("åŸºé‡‘ %s å·²è·å–æ‰€æœ‰æ–°æ•°æ®ï¼Œçˆ¬å–ç»“æŸã€‚", fund_code)
                            break
                        # å¦‚æœå½“å‰é¡µæ²¡æœ‰æ–°æ•°æ®ï¼Œä¸”æ˜¯ç¬¬ä¸€é¡µï¼Œåˆ™è¯´æ˜æ²¡æœ‰æ–°æ•°æ®
                        elif page_index == 1:
                            logger.info("åŸºé‡‘ %s æ— æ–°æ•°æ®ï¼Œçˆ¬å–ç»“æŸã€‚", fund_code)
                            break
                    else:
                        has_new_data = True
                        all_new_data.append(new_df_page)
                        logger.info("ç¬¬ %d é¡µ: å‘ç° %d è¡Œæ–°æ•°æ®", page_index, len(new_df_page))
                else:
                    # å¦‚æœæ˜¯é¦–æ¬¡ä¸‹è½½ï¼Œåˆ™è·å–æ‰€æœ‰æ•°æ®
                    all_new_data.append(df_page)

                logger.info("åŸºé‡‘ %s æ€»é¡µæ•°: %d, å½“å‰é¡µ: %d, å½“å‰é¡µè¡Œæ•°: %d", fund_code, total_pages, page_index, len(df_page))
                
                # å¦‚æœæ˜¯å¢é‡æ›´æ–°æ¨¡å¼ï¼Œä¸”å½“å‰é¡µæ•°æ®æ¯”æœ€æ–°æ•°æ®æ—¥æœŸæ—©ï¼Œåˆ™ç»“æŸå¾ªç¯
                if latest_local_date and (df_page['date'].dt.date <= latest_local_date).any():
                    logger.info("åŸºé‡‘ %s å·²è¿½æº¯åˆ°æœ¬åœ°æ•°æ®ï¼Œå¢é‡çˆ¬å–ç»“æŸã€‚", fund_code)
                    break

                if page_index >= total_pages:
                    logger.info("åŸºé‡‘ %s å·²è·å–æ‰€æœ‰å†å²æ•°æ®ï¼Œå…± %d é¡µï¼Œçˆ¬å–ç»“æŸ", fund_code, total_pages)
                    break
                
                page_index += 1
                time_module.sleep(random.uniform(self.sleep_min, self.sleep_max))  # ä½¿ç”¨é…ç½®çš„sleepæ—¶é—´
                
            except requests.exceptions.RequestException as e:
                logger.error("åŸºé‡‘ %s APIè¯·æ±‚å¤±è´¥: %s", fund_code, str(e))
                raise
            except Exception as e:
                logger.error("åŸºé‡‘ %s APIæ•°æ®è§£æå¤±è´¥: %s", fund_code, str(e))
                raise

        # åˆå¹¶æ–°æ•°æ®å¹¶è¿”å›
        if all_new_data:
            new_combined_df = pd.concat(all_new_data, ignore_index=True)
            # éªŒè¯åˆå¹¶åçš„æ•°æ®
            is_valid, error_msg = self._validate_fund_data(new_combined_df, fund_code)
            if not is_valid:
                logger.error("åˆå¹¶åçš„æ–°æ•°æ®éªŒè¯å¤±è´¥: %s", error_msg)
                return pd.DataFrame()
            return new_combined_df[['date', 'net_value']]
        else:
            return pd.DataFrame()

    def _load_index_data_from_file(self):
        """
        ä»æœ¬åœ°æ–‡ä»¶åŠ è½½å¤§ç›˜æŒ‡æ•°æ•°æ®
        """
        file_path = os.path.join(INDEX_DATA_DIR, f"{self.index_code}.csv")
        logger.info("æ­£åœ¨ä»æœ¬åœ°æ–‡ä»¶ %s åŠ è½½å¤§ç›˜æŒ‡æ•°æ•°æ®...", file_path)
        if not os.path.exists(file_path):
            logger.error("æœ¬åœ°å¤§ç›˜æŒ‡æ•°æ–‡ä»¶ %s ä¸å­˜åœ¨ï¼Œè¯·è¿è¡Œ download_index_data.py ä¸‹è½½ã€‚", file_path)
            return pd.DataFrame()
        
        try:
            df = pd.read_csv(file_path, parse_dates=['date'])
            # éªŒè¯æŒ‡æ•°æ•°æ®
            is_valid, error_msg = self._validate_fund_data(df, self.index_code)
            if not is_valid:
                logger.error("å¤§ç›˜æŒ‡æ•°æ•°æ®éªŒè¯å¤±è´¥: %s", error_msg)
                return pd.DataFrame()
            
            df = df.sort_values(by='date', ascending=True).reset_index(drop=True)
            logger.info("å¤§ç›˜æŒ‡æ•° %s æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± %d è¡Œï¼Œæœ€æ–°æ—¥æœŸä¸º: %s", self.index_code, len(df), df['date'].max().date())
            return df
        except Exception as e:
            logger.error("åŠ è½½æœ¬åœ°å¤§ç›˜æŒ‡æ•°æ–‡ä»¶ %s å¤±è´¥: %s", file_path, e)
            return pd.DataFrame()

    def _calculate_indicators(self, df):
        """è®¡ç®—æŠ€æœ¯æŒ‡æ ‡å¹¶ç”Ÿæˆç»“æœå­—å…¸"""
        if df is None or df.empty or len(df) < self.min_data_points:
            return None

        df = df.sort_values(by='date', ascending=True)
        
        # MACD - ä½¿ç”¨é…ç½®å‚æ•°
        exp_fast = df['net_value'].ewm(span=self.macd_fast, adjust=False).mean()
        exp_slow = df['net_value'].ewm(span=self.macd_slow, adjust=False).mean()
        df['macd'] = exp_fast - exp_slow
        df['signal'] = df['macd'].ewm(span=self.macd_signal, adjust=False).mean()

        # å¸ƒæ—å¸¦ - ä½¿ç”¨é…ç½®å‚æ•°
        df['bb_mid'] = df['net_value'].rolling(window=self.bollinger_window, min_periods=1).mean()
        df['bb_std'] = df['net_value'].rolling(window=self.bollinger_window, min_periods=1).std()
        df['bb_upper'] = df['bb_mid'] + (df['bb_std'] * self.bollinger_std)
        df['bb_lower'] = df['bb_mid'] - (df['bb_std'] * self.bollinger_std)
        
        # RSI - ä½¿ç”¨é…ç½®å‚æ•°
        delta = df['net_value'].diff()
        gain = delta.where(delta > 0, 0)
        loss = -delta.where(delta < 0, 0)
        
        avg_gain = gain.rolling(window=self.rsi_window, min_periods=1).mean()
        avg_loss = loss.rolling(window=self.rsi_window, min_periods=1).mean()
        
        rs = avg_gain / avg_loss.replace(0, np.nan)
        df['rsi'] = 100 - (100 / (1 + rs))

        # MA - ä½¿ç”¨é…ç½®å‚æ•°
        df['ma50'] = df['net_value'].rolling(window=min(self.ma_window, len(df)), min_periods=1).mean()
        df['ma_ratio'] = df['net_value'] / df['ma50']

        return df

    def _get_latest_signals(self, fund_code, df):
        """æ ¹æ®æœ€æ–°æ•°æ®è®¡ç®—ä¿¡å·"""
        try:
            processed_df = self._calculate_indicators(df)
            if processed_df is None:
                logger.warning("åŸºé‡‘ %s æ•°æ®ä¸è¶³ï¼Œè·³è¿‡è®¡ç®—", fund_code)
                return {
                    'fund_code': fund_code, 'latest_net_value': "æ•°æ®è·å–å¤±è´¥", 'rsi': np.nan, 'ma_ratio': np.nan,
                    'macd_diff': np.nan, 'bb_upper': np.nan, 'bb_lower': np.nan, 'bb_position': 'N/A', 'advice': "è§‚å¯Ÿ", 'action_signal': 'N/A'
                }
            
            latest_data = processed_df.iloc[-1]
            latest_net_value = latest_data['net_value']
            latest_rsi = latest_data['rsi']
            latest_ma50_ratio = latest_data['ma_ratio']
            latest_macd_diff = latest_data['macd'] - latest_data['signal']
            latest_bb_upper = latest_data['bb_upper']
            latest_bb_lower = latest_data['bb_lower']

            advice = "è§‚å¯Ÿ"
            if (not np.isnan(latest_rsi) and latest_rsi > self.rsi_overbought) or \
               (not np.isnan(latest_bb_upper) and latest_net_value > latest_bb_upper) or \
               (not np.isnan(latest_ma50_ratio) and latest_ma50_ratio > self.ma_ratio_high):
                advice = "ç­‰å¾…å›è°ƒ"
            elif (not np.isnan(latest_rsi) and latest_rsi < self.rsi_oversold) or \
                 (not np.isnan(latest_bb_lower) and latest_net_value < latest_bb_lower) or \
                 (not np.isnan(latest_ma50_ratio) and latest_ma50_ratio < self.ma_ratio_low):
                advice = "å¯åˆ†æ‰¹ä¹°å…¥"
            elif (not np.isnan(latest_ma50_ratio) and latest_ma50_ratio > 1) and \
                 (not np.isnan(latest_macd_diff) and latest_macd_diff > 0):
                advice = "å¯åˆ†æ‰¹ä¹°å…¥"
            elif (not np.isnan(latest_ma50_ratio) and latest_ma50_ratio < 1) and \
                 (not np.isnan(latest_macd_diff) and latest_macd_diff < 0):
                advice = "ç­‰å¾…å›è°ƒ"

            action_signal = "æŒæœ‰/è§‚å¯Ÿ"
            if not np.isnan(latest_ma50_ratio) and latest_ma50_ratio < 0.95:
                action_signal = "å¼ºå–å‡º/è§„é¿"
            elif (not np.isnan(latest_rsi) and latest_rsi > self.rsi_overbought) and \
                 (not np.isnan(latest_ma50_ratio) and latest_ma50_ratio > self.ma_ratio_strong_sell) and \
                 (not np.isnan(latest_macd_diff) and latest_macd_diff < 0):
                action_signal = "å¼ºå–å‡º/è§„é¿"
            elif (not np.isnan(latest_rsi) and latest_rsi > self.rsi_sell_weak) or \
                 (not np.isnan(latest_bb_upper) and latest_net_value > latest_bb_upper) or \
                 (not np.isnan(latest_ma50_ratio) and latest_ma50_ratio > self.ma_ratio_strong_sell):
                action_signal = "å¼±å–å‡º/è§„é¿"
            elif (not np.isnan(latest_rsi) and latest_rsi < self.rsi_buy_strong) and \
                 (not np.isnan(latest_ma50_ratio) and latest_ma50_ratio < self.ma_ratio_strong_buy) and \
                 (not np.isnan(latest_macd_diff) and latest_macd_diff > 0):
                action_signal = "å¼ºä¹°å…¥"
            elif (not np.isnan(latest_rsi) and latest_rsi < self.rsi_buy_threshold) or \
                 (not np.isnan(latest_bb_lower) and latest_net_value < latest_bb_lower) or \
                 (not np.isnan(latest_ma50_ratio) and latest_ma50_ratio < self.ma_ratio_buy_threshold):
                action_signal = "å¼±ä¹°å…¥"
            
            # è®¡ç®—å¸ƒæ—å¸¦ä½ç½®
            bb_position = "ä¸­è½¨"
            if not np.isnan(latest_net_value) and not np.isnan(latest_bb_upper) and latest_net_value > latest_bb_upper:
                bb_position = "ä¸Šè½¨ä¸Šæ–¹"
            elif not np.isnan(latest_net_value) and not np.isnan(latest_bb_lower) and latest_net_value < latest_bb_lower:
                bb_position = "ä¸‹è½¨ä¸‹æ–¹"

            return {
                'fund_code': fund_code,
                'latest_net_value': latest_net_value,
                'rsi': latest_rsi,
                'ma_ratio': latest_ma50_ratio,
                'macd_diff': latest_macd_diff,
                'bb_upper': latest_bb_upper,
                'bb_lower': latest_bb_lower,
                'bb_position': bb_position,
                'advice': advice,
                'action_signal': action_signal
            }
        except Exception as e:
            logger.error("å¤„ç†åŸºé‡‘ %s æ—¶å‘ç”Ÿå¼‚å¸¸: %s", fund_code, str(e))
            return {
                'fund_code': fund_code, 'latest_net_value': "æ•°æ®è·å–å¤±è´¥", 'rsi': np.nan, 'ma_ratio': np.nan,
                'macd_diff': np.nan, 'bb_upper': np.nan, 'bb_lower': np.nan, 'bb_position': 'N/A',
                'advice': "è§‚å¯Ÿ", 'action_signal': 'N/A'
            }

    def _process_single_fund(self, fund_code):
        """å¤„ç†å•ä¸ªåŸºé‡‘æ•°æ®ï¼šè¯»å–æœ¬åœ°ï¼Œä¸‹è½½å¢é‡ï¼Œåˆå¹¶ï¼Œä¿å­˜ï¼Œå¹¶è®¡ç®—ä¿¡å·"""
        local_df = self._read_local_data(fund_code)
        latest_local_date = local_df['date'].max().date() if not local_df.empty else None
        
        new_df = self._fetch_fund_data(fund_code, latest_local_date)
        
        if not new_df.empty:
            df_final = pd.concat([local_df, new_df]).drop_duplicates(subset=['date'], keep='last').sort_values(by='date', ascending=True)
            if self._save_to_local_file(fund_code, df_final):
                return self._get_latest_signals(fund_code, df_final.tail(100))
            else:
                logger.error("åŸºé‡‘ %s ä¿å­˜å¤±è´¥ï¼Œè·³è¿‡", fund_code)
                return None
        elif not local_df.empty:
            # å¦‚æœæ²¡æœ‰æ–°æ•°æ®ï¼Œä¸”æœ¬åœ°æœ‰æ•°æ®ï¼Œåˆ™ä½¿ç”¨æœ¬åœ°æ•°æ®è®¡ç®—ä¿¡å·
            logger.info("åŸºé‡‘ %s æ— æ–°æ•°æ®ï¼Œä½¿ç”¨æœ¬åœ°å†å²æ•°æ®è¿›è¡Œåˆ†æ", fund_code)
            return self._get_latest_signals(fund_code, local_df.tail(100))
        else:
            logger.error("åŸºé‡‘ %s æ— å¯ç”¨æ•°æ®ï¼Œè·³è¿‡", fund_code)
            return None

    def _analyze_index(self, index_df):
        """åˆ†æå¤§ç›˜æŒ‡æ•°ï¼Œè®¡ç®—æŠ€æœ¯æŒ‡æ ‡å’Œä¿¡å·"""
        logger.info("å¼€å§‹åˆ†æå¤§ç›˜æŒ‡æ•° %s...", self.index_code)
        if not index_df.empty:
            self.index_df = index_df
            result = self._get_latest_signals(self.index_code, self.index_df.tail(100))
            self.index_data = result
            logger.info("å¤§ç›˜æŒ‡æ•° %s åˆ†æå®Œæˆ", self.index_code)
        else:
            logger.error("å¤§ç›˜æŒ‡æ•° %s æœªè·å–åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®", self.index_code)
            self.index_data = {
                'fund_code': self.index_code, 'latest_net_value': "æ•°æ®è·å–å¤±è´¥", 'rsi': np.nan, 'ma_ratio': np.nan,
                'macd_diff': np.nan, 'bb_upper': np.nan, 'bb_lower': np.nan, 'bb_position': 'N/A', 'advice': "è§‚å¯Ÿ", 'action_signal': 'N/A'
            }
            
    def _get_portfolio_signals(self, fund_data, max_positions=None):
        """ æ ¹æ®ç»¼åˆè¯„åˆ†ç­›é€‰å‡ºå€¼å¾—ä¹°å…¥çš„åŸºé‡‘ """
        if max_positions is None:
            max_positions = self.max_positions

        buy_signals = []
        for code, data in fund_data.items():
            if data and data['action_signal'] in ["å¼ºä¹°å…¥", "å¼±ä¹°å…¥"] and not np.isnan(data['rsi']):
                score = self._calculate_buy_score(data)
                buy_signals.append({
                    'code': code,
                    'signal': data['action_signal'],
                    'score': score,
                    'rsi': data['rsi'],
                    'ma_ratio': data['ma_ratio']
                })
        # æŒ‰ç…§è¯„åˆ†é™åºæ’åˆ—ï¼Œå–å‰Nä¸ª
        buy_signals = sorted(buy_signals, key=lambda x: x['score'], reverse=True)
        return buy_signals[:max_positions]
    
    def _calculate_buy_score(self, data):
        """
        è®¡ç®—åŸºé‡‘ä¹°å…¥è¯„åˆ†
        RSIè¶Šä½åˆ†æ•°è¶Šé«˜ï¼ŒMA_Ratioè¶Šä½åˆ†æ•°è¶Šé«˜ï¼Œå¸ƒæ—å¸¦ä½ç½®è¶Šä½åˆ†æ•°è¶Šé«˜
        """
        score = 0
        # 1. RSIè¯„åˆ†: 40åˆ† - ä½¿ç”¨é…ç½®çš„é˜ˆå€¼
        if data['rsi'] < self.rsi_oversold:
            score += 40
        elif data['rsi'] < self.rsi_buy_strong:
            score += 30
        elif data['rsi'] < self.rsi_buy_threshold:
            score += 20
        
        # 2. MA_Ratioè¯„åˆ†: 40åˆ† - ä½¿ç”¨é…ç½®çš„é˜ˆå€¼
        if data['ma_ratio'] < self.ma_ratio_strong_buy:
            score += 40
        elif data['ma_ratio'] < self.ma_ratio_low:
            score += 30
        elif data['ma_ratio'] < self.ma_ratio_buy_threshold:
            score += 20
        elif data['ma_ratio'] < 1.05:
            score += 10

        # 3. å¸ƒæ—å¸¦ä½ç½®è¯„åˆ†: 20åˆ†
        bb_position = data['bb_position']
        if bb_position == "ä¸‹è½¨ä¸‹æ–¹":
            score += 20
        elif bb_position == "ä¸­è½¨":
            score += 10
        else:
            score += 5
        
        return score
    
    def generate_portfolio_recommendation(self):
        """ç”ŸæˆæŠ•èµ„ç»„åˆæ¨è"""
        buy_candidates = self._get_portfolio_signals(self.fund_data, max_positions=3)
        with open(self.portfolio_output_file, 'w', encoding='utf-8') as f:
            f.write(f"# æŠ•èµ„ç»„åˆæ¨èæŠ¥å‘Š ({datetime.now().strftime('%Y-%m-%d')})\n\n")
            f.write("---")
            f.write(f"### å¤§ç›˜æŒ‡æ•° {self.index_code} å¸‚åœºæƒ…ç»ª\n\n")
            f.write(f"ğŸ“ˆ **æœ€æ–°å‡€å€¼**: {float(self.index_data['latest_net_value']):.2f}\n")
            f.write(f"ğŸ“Š **RSI**: {self.index_data['rsi']:.2f}\n")
            f.write(f"ğŸ“‰ **MA_Ratio**: {self.index_data['ma_ratio']:.2f}\n")
            f.write(f"ğŸ’¡ **å½“å‰ä¿¡å·**: {self.index_data['action_signal']} | {self.index_data['advice']}\n")
            f.write("---")
            f.write("\n## æ¨èåŸºé‡‘åˆ—è¡¨\n\n")
            if buy_candidates:
                f.write("| åºå· | ä¿¡å· | åŸºé‡‘ä»£ç  | è¯„åˆ† | RSI | MA_Ratio |\n")
                f.write("|------|------|----------|------|-----|----------|\n")
                for i, candidate in enumerate(buy_candidates, 1):
                    signal_emoji = "ğŸŸ¢ å¼ºä¹°å…¥" if candidate['signal'] == "å¼ºä¹°å…¥" else "ğŸŸ¡ å¼±ä¹°å…¥"
                    f.write(f"| {i} | {signal_emoji} | {candidate['code']} | {candidate['score']:.0f} | {candidate['rsi']:.1f} | {candidate['ma_ratio']:.2f} |\n")
                if buy_candidates:
                    suggested_amount = buy_candidates[0]['score'] // 10 * self.suggested_allocation_base
                    f.write(f"\n## å»ºè®®åˆ†é…\n")
                    f.write(f"ğŸ’° å»ºè®®æ¯æ”¯åŸºé‡‘åˆ†é…: {suggested_amount} å…ƒ\n\n")
                f.write(f"ğŸ“ˆ ä»Šæ—¥ä¹°å…¥æœºä¼š: {len(buy_candidates)} / {len(self.fund_codes)}\n\n")
            else:
                f.write("## æ¨èç»“æœ\n")
                f.write("âŒ ä»Šæ—¥æ— ç¬¦åˆæ¡ä»¶çš„ä¹°å…¥æœºä¼šï¼Œå»ºè®®è§‚æœ›\n\n")
                f.write(f"ğŸ“Š æ€»æ‰«æåŸºé‡‘æ•°: {len(self.fund_codes)}\n\n")
            logger.info("æŠ•èµ„ç»„åˆæ¨èæŠ¥å‘Šç”Ÿæˆå®Œæˆ: %s", self.portfolio_output_file)

    def generate_detailed_report(self):
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        logger.info("æ­£åœ¨ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š: %s", self.output_file)
        with open(self.output_file, 'w', encoding='utf-8') as f:
            f.write(f"# å¸‚åœºç›‘æ§æŠ¥å‘Š ({datetime.now().strftime('%Y-%m-%d')})\n\n")
            f.write("---")
            f.write(f"### å¤§ç›˜æŒ‡æ•° {self.index_code} å¸‚åœºæƒ…ç»ª\n\n")
            f.write(f"ğŸ“ˆ **æœ€æ–°å‡€å€¼**: {float(self.index_data['latest_net_value']):.2f}\n")
            f.write(f"ğŸ“Š **RSI**: {self.index_data['rsi']:.2f}\n")
            f.write(f"ğŸ“‰ **MA_Ratio**: {self.index_data['ma_ratio']:.2f}\n")
            f.write(f"ğŸ’¡ **å½“å‰ä¿¡å·**: {self.index_data['action_signal']} | {self.index_data['advice']}\n")
            f.write("---\n")
            f.write("\n## åŸºé‡‘æ•°æ®åˆ†æ\n\n")
            f.write("| åŸºé‡‘ä»£ç  | æœ€æ–°å‡€å€¼ | RSI | MA50æ¯”ä¾‹ | MACDä¿¡å· | å¸ƒæ—å¸¦ä½ç½® | å»ºè®® | æ“ä½œä¿¡å· |\n")
            f.write("|----------|----------|-----|----------|----------|------------|------|----------|\n")
            
            for fund_code, data in self.fund_data.items():
                if isinstance(data, dict):
                    macd_signal = "N/A"
                    if not np.isnan(data['macd_diff']):
                        macd_signal = "é‡‘å‰" if data['macd_diff'] > 0 else "æ­»å‰"
                    
                    f.write(
                        f"| {fund_code} | {float(data['latest_net_value']):.2f} | {data['rsi']:.2f} | {data['ma_ratio']:.2f} | {macd_signal} | {data['bb_position']} | {data['advice']} | {data['action_signal']} |\n"
                    )
                else:
                    f.write(f"| {fund_code} | æ•°æ®è·å–å¤±è´¥ | N/A | N/A | N/A | N/A | è§‚å¯Ÿ | N/A |\n")

        logger.info("è¯¦ç»†æŠ¥å‘Šç”Ÿæˆå®Œæˆ: %s", self.output_file)

    def _generate_backtest_report(self):
        # æ­¤åŠŸèƒ½å·²ç§»é™¤
        pass

    def run(self):
        """
        ä¸»æ‰§è¡Œå‡½æ•°ï¼Œåè°ƒæ‰€æœ‰æ“ä½œ
        """
        logger.info("è„šæœ¬å¼€å§‹æ‰§è¡Œï¼Œå½“å‰æ—¶é—´ä¸º: %s", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
        
        try:
            # 1. è§£ææŠ¥å‘Šæ–‡ä»¶è·å–åŸºé‡‘ä»£ç 
            self._parse_report()

            # 2. åŠ è½½å¤§ç›˜æŒ‡æ•°æ•°æ®
            index_df = self._load_index_data_from_file()
            self._analyze_index(index_df)

            # 3. å¤šçº¿ç¨‹å¤„ç†æ¯ä¸ªåŸºé‡‘
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_fund = {executor.submit(self._process_single_fund, fund_code): fund_code for fund_code in self.fund_codes}
                for future in concurrent.futures.as_completed(future_to_fund):
                    fund_code = future_to_fund[future]
                    try:
                        data = future.result()
                        if data:
                            self.fund_data[fund_code] = data
                    except Exception as e:
                        logger.error("å¤„ç†åŸºé‡‘ %s æ—¶å‘ç”Ÿå¼‚å¸¸: %s", fund_code, e)
                        self.fund_data[fund_code] = None
            
            # 4. ç”ŸæˆæŠ¥å‘Š
            self.generate_portfolio_recommendation()
            self.generate_detailed_report()

        except Exception as e:
            logger.exception("è„šæœ¬æ‰§è¡Œå¤±è´¥: %s", e)

if __name__ == '__main__':
    monitor = MarketMonitor()
    monitor.run()
